{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a396014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_day import download_day\n",
    "from load_saved_fd4 import load_saved_fd4\n",
    "from scrape_aircraft_list import scrape_aircraft_list\n",
    "from get_perf_model_typecodes import get_perf_model_typecodes \n",
    "from match_icao_model import match_icao_model\n",
    "from process_airport_list import process_airport_list\n",
    "from generate_flightpath import generate_flightpath\n",
    "from plot_flightpaths import plot_flightpaths\n",
    "from get_engine_data import get_engine_data\n",
    "from perf_model_powerplant_parser import perf_model_powerplant_parser\n",
    "from match_engine_to_emissions_db import match_engine_to_emissions_db\n",
    "from process_month_emissions import process_month_emissions\n",
    "from get_era5_wind import get_era5_wind\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "from time import sleep\n",
    "from geopy import distance\n",
    "import requests\n",
    "from icet import icet\n",
    "from bffm2 import bffm2\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# User Inputs:\n",
    "start_time_str       = '2023-01-01T00:00:00Z'\n",
    "stop_time_str        = '2023-12-31T23:59:59Z'\n",
    "query_limit          = 15e4\n",
    "send_notification    = True\n",
    "make_plot            = True\n",
    "output_dir           = \"/scratch/omg28/Data/\"\n",
    "\n",
    "# Convert start and stop times to datetime objects\n",
    "start_time_simple = pd.to_datetime(start_time_str).strftime(\"%Y-%m-%d\")\n",
    "stop_time_simple = pd.to_datetime(stop_time_str).strftime(\"%Y-%m-%d\")\n",
    "analysis_year = pd.to_datetime(start_time_str).year\n",
    "\n",
    "# Define grid\n",
    "lat_bins = np.arange(-90, 90.1, 0.5)\n",
    "lon_bins = np.arange(-180, 180.1, 0.5)\n",
    "alt_bins_ft = np.arange(0, 55001, 1000)\n",
    "alt_bins_m = alt_bins_ft * 0.3048\n",
    "nlat, nlon, nalt = len(lat_bins)-1, len(lon_bins)-1, len(alt_bins_m)-1\n",
    "\n",
    "# Define countries whose airspace we want to exclude\n",
    "conflict_countries = ['Russia', 'Ukraine', 'Libya', 'Syria', 'Sudan', 'Yemen']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4dd1a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Generate the file pattern for all daily flight data files in 2023\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m file_pattern = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43moutput_dir\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mno_track2023/result_df_2023-*.pkl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m flight_files = glob.glob(file_pattern)\n\u001b[32m      7\u001b[39m total_flights = \u001b[32m0\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'output_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Generate the file pattern for all daily flight data files in 2023\n",
    "file_pattern = f\"{output_dir}no_track2023/result_df_2023-*.pkl\"\n",
    "flight_files = glob.glob(file_pattern)\n",
    "\n",
    "total_flights = 0\n",
    "for file in flight_files:\n",
    "    try:\n",
    "        df = pd.read_pickle(file)\n",
    "        total_flights += len(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read {file}: {e}\")\n",
    "\n",
    "print(f\"Total number of flights tracked in 2023: {total_flights:,}\")\n",
    "\n",
    "############### Count flights missing departure or arrival airport information##########\n",
    "missing_airport_flights = 0\n",
    "for file in flight_files:\n",
    "    try:\n",
    "        df = pd.read_pickle(file)\n",
    "        # Count rows where either estdepartureairport or estarrivalairport is missing/null/empty\n",
    "        missing = df['estdepartureairport'].isnull() | df['estarrivalairport'].isnull() | \\\n",
    "                  (df['estdepartureairport'] == '') | (df['estarrivalairport'] == '')\n",
    "        missing_airport_flights += missing.sum()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read {file}: {e}\")\n",
    "\n",
    "if total_flights > 0:\n",
    "    proportion_missing = missing_airport_flights / total_flights\n",
    "    print(f\"Flights missing departure or arrival airport: {missing_airport_flights:,} ({proportion_missing:.2%})\")\n",
    "else:\n",
    "    print(\"No flights found to analyze missing airport information.\")\n",
    "############ Counting flights in typecodes_added files for 2023 ############\n",
    "# Count flights in typecodes_added files for 2023\n",
    "# (glob and pandas are already imported above)\n",
    "\n",
    "# Pattern for all monthly typecodes_added files for 2023\n",
    "pattern = f\"{output_dir}aircraftdb/2023-*_to_2023-*_150000_typecodes_added.pkl\"\n",
    "files = sorted(glob.glob(pattern))\n",
    "\n",
    "total_typecodes_flights = 0\n",
    "monthly_counts = {}\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_pickle(file)\n",
    "    n = len(df)\n",
    "    total_typecodes_flights += n\n",
    "    # Extract month for reporting\n",
    "    month = file.split('/')[-1].split('_')[0][5:7]\n",
    "    monthly_counts[month] = n\n",
    "\n",
    "print(f\"Total number of flights in all typecodes_added files: {total_typecodes_flights:,}\")\n",
    "\n",
    "# Calculate and report the percentage of flights NOT in typecodes_added files\n",
    "if total_flights > 0:\n",
    "    missing_typecode_flights = total_flights - total_typecodes_flights\n",
    "    percent_missing = (missing_typecode_flights / total_flights) * 100\n",
    "    print(f\"Flights missing typecode: {missing_typecode_flights:,} ({percent_missing:.2f}%)\")\n",
    "else:\n",
    "    print(\"No flights found to analyze typecodes_added coverage.\")\n",
    "\n",
    "\n",
    "# Count the number and proportion of flights in 2023 done by helicopters\n",
    "helicopter_flights = 0\n",
    "for file in files:\n",
    "    df = pd.read_pickle(file)\n",
    "    # ICAO typecodes for helicopters typically start with 'H'\n",
    "    helicopter_flights += df['typecode'].astype(str).str.startswith('H').sum()\n",
    "\n",
    "print(f\"Number of helicopter flights in 2023: {helicopter_flights:,}\")\n",
    "\n",
    "if total_flights > 0:\n",
    "    helicopter_proportion = helicopter_flights / total_flights\n",
    "    print(f\"Proportion of total flights in 2023 done by helicopters: {helicopter_proportion:.2%}\")\n",
    "else:\n",
    "    print(\"No flights found to analyze helicopter flights.\")\n",
    "\n",
    "eucontrol_typecodes = pd.read_csv(\"performance_models_typecodes.csv\")['typecode'].unique()\n",
    "\n",
    "eucontrol_flights = 0\n",
    "for file in files:\n",
    "    df = pd.read_pickle(file)\n",
    "    eucontrol_flights += df['typecode'].isin(eucontrol_typecodes).sum()\n",
    "\n",
    "print(f\"Number of flights in 2023 with a typecode in the EUControl database: {eucontrol_flights:,}\")\n",
    "\n",
    "if total_flights > 0:\n",
    "    eucontrol_proportion = eucontrol_flights / total_flights\n",
    "    print(f\"Proportion of total flights in 2023 with EUControl typecode: {eucontrol_proportion:.2%}\")\n",
    "else:\n",
    "    print(\"No flights found to analyze EUControl typecode coverage.\")\n",
    "    \n",
    "# count the total number or flights in 2023 used in the emissions model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e008886",
   "metadata": {},
   "source": [
    "# Section 1. Download flight information from Opensky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278aa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download data from OpenSky history database\n",
    "# WARNING: This may take a long time to run and will use a lot of disk space. Recommend running this in a .py script to avoid recurring memory issues found in Jupyter notebooks.\n",
    "# download_day(start_time_str, stop_time_str, query_limit, send_notification, make_plot, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c13875",
   "metadata": {},
   "source": [
    "# Section 2. Scrape the EUCONTROL database for aircraft flight performance information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# scrape the list of aircraft with performance models from the EUCONTROL website\n",
    "perf_model_typecodes = get_perf_model_typecodes()\n",
    "## WARNING: THIS CAN GET YOUR IP ADDRESS BLOCKED IF YOU RUN IT REPEATEDLY. IT IS A WEB SCRAPER.\n",
    "aircraft_list = scrape_aircraft_list(perf_model_typecodes)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e24ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# get the aircraft metadata database from the OpenSky network\n",
    "os.system(f'wget -P {output_dir}aircraftdb https://s3.opensky-network.org/data-samples/metadata/aircraft-database-complete-2025-02.csv')\n",
    "\n",
    "# get airport list from the ourairports database\n",
    "os.system(f'wget -P {output_dir}airportdb https://davidmegginson.github.io/ourairports-data/airports.csv')\n",
    "\n",
    "# Download the ICAO engine database from easa database and save it as a CSV file\n",
    "xlsx_url = \"https://www.easa.europa.eu/en/downloads/131424/en\"\n",
    "xlsx_path = \"/scratch/omg28/Data/engine_data/engine_data_icao.xlsx\"\n",
    "csv_path = \"/scratch/omg28/Data/engine_data/engine_data_icao.csv\"\n",
    "\n",
    "# Download the file if it doesn't exist\n",
    "if not os.path.exists(xlsx_path):\n",
    "    os.makedirs(os.path.dirname(xlsx_path), exist_ok=True)\n",
    "    r = requests.get(xlsx_url)\n",
    "    with open(xlsx_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "# Read the specified sheet and save as CSV\n",
    "df = pd.read_excel(xlsx_path, sheet_name='Gaseous Emissions and Smoke')\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Get ERA5 wind reanalysis data for the specified time period\n",
    "netcdf_path = get_era5_wind(analysis_year, output_dir)\n",
    "\n",
    "# Download the country shapefiles\n",
    "os.system(f'wget https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_0_countries.zip')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the helicopters from the aircraft performance database list\n",
    "perf_models = pd.read_csv(\"aircraft_performance_table.csv\")\n",
    "perf_models = perf_models[~perf_models['type'].str.contains('H')]\n",
    "perf_models.to_csv(\"aircraft_performance_table_filtered.csv\", index=False)\n",
    "pd.to_pickle(perf_models, \"aircraft_performance_table_filtered.pkl\")\n",
    "num_helicopters = pd.read_csv(\"aircraft_performance_table.csv\")['type'].str.contains('H').sum()\n",
    "print(f\"Number of helicopters in the original database: {num_helicopters}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e861cc1",
   "metadata": {},
   "source": [
    "# Section 3. Process the aircraft performance and flight data, retaining only flights we have takeoff, landing, plane type, and aircraft performance model information about. Further, drop all flights with aircraft codes that do not correspond to a code in the ourairports registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1246e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the loaded time period of data. - 12s\n",
    "# - matches 24 bit transponder codes from Feb 2025 aircraft metadata database with the ICAO typecode\n",
    "# - removes all flights for which the transponder code does not have a corresponding ICAO typecode\n",
    "# - removes all flights for which the takeoff OR landing airport is unknown\n",
    "def process_month(start_time_str_loop):\n",
    "    # Set end date to last day of month at 23:59:59 UTC\n",
    "    stop_time_str_loop = (start_time_str_loop + pd.offsets.MonthEnd(1)).replace(hour=23, minute=59, second=59)\n",
    "    start_time_simple_loop = pd.to_datetime(start_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    stop_time_simple_loop = pd.to_datetime(stop_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    print(f\"Processing data from {start_time_str_loop} to {stop_time_str_loop}\")\n",
    "\n",
    "    flights_with_typecode,a,b,c,d,e,f,g,h,i,j,k = match_icao_model(str(start_time_str_loop), str(stop_time_str_loop), query_limit, \n",
    "                                            aircraft_db_path = \"/scratch/omg28/Data/aircraftdb/aircraft-database-complete-2025-02.csv\",\n",
    "                                            flight_db_path = \"/scratch/omg28/Data/no_track2023/\",\n",
    "                                            output_dir = \"/scratch/omg28/Data/aircraftdb/\")\n",
    "    \n",
    "    # Load in the list of ICAO typecodes that have a performance model - 7.5s\n",
    "    performance_model_typecodes = pd.read_pickle('/scratch/omg28/Data/aircraftdb/performance_models_typecodes.pkl')\n",
    "\n",
    "    # load in the list of all flights with typecode and takeoff/landing airport - uncomment this line to load the data from a .pkl file\n",
    "    # instead of running the match_icao_model function\n",
    "    # flights_with_typecode = pd.read_pickle(f'/scratch/omg28/Data/aircraftdb/{start_time_simple_loop}_to_{stop_time_simple_loop}_{int(query_limit)}_typecodes_added.pkl')\n",
    "\n",
    "    # get info before the merge\n",
    "    print(\"Before merging:\")\n",
    "    print(flights_with_typecode.info())\n",
    "\n",
    "    # inner join the loaded year of data with the scraped aircraft list\n",
    "    # - this will remove all flights for which the ICAO typecode does not have a corresponding performance model\n",
    "    flights_with_perf_typecode = pd.merge(flights_with_typecode, performance_model_typecodes, how='inner', on = 'typecode')\n",
    "\n",
    "    print(\"After merging:\")\n",
    "    print(flights_with_perf_typecode.info())\n",
    "\n",
    "    # print a list of typecodes in complete_flights_perf_model that do not have a performance model\n",
    "    missing_perf_models = flights_with_typecode[~flights_with_typecode['typecode'].isin(performance_model_typecodes['typecode'])]\n",
    "    print(\"Aircraft models with observed flights, but no performance model: \" + str(len(missing_perf_models['typecode'].unique())))\n",
    "    print(\"total flights in 2024 with TOLD and typecode, but no performance model: \" + str(len(missing_perf_models)))\n",
    "\n",
    "    # print a list of typecodes in performance_model_typecodes that do not correspond to a flight\n",
    "    missing_flights = performance_model_typecodes[~performance_model_typecodes['typecode'].isin(flights_with_typecode['typecode'])]\n",
    "    print(\"Number of performance models with no flights: \" + str(len(missing_flights['typecode'].unique())))\n",
    "    print(\"Number of flights with TOLD, typecode, and performance model: \" + str(len(flights_with_perf_typecode)))\n",
    "\n",
    "    # count all flights starting and ending at the same airport\n",
    "    num_flights_no_dist = len(flights_with_perf_typecode[flights_with_perf_typecode['estdepartureairport'] == flights_with_perf_typecode['estarrivalairport']])\n",
    "    print(\"Number of flights with no distance: \" + str(num_flights_no_dist))\n",
    "\n",
    "    # remove all flights starting or ending at the same airport\n",
    "    flights_with_perf_typecode_dist = flights_with_perf_typecode[flights_with_perf_typecode['estdepartureairport'] != flights_with_perf_typecode['estarrivalairport']]\n",
    "    print(\"Number of flights with TOLD, typecode, performance model, and nonzero distance: \" + str(len(flights_with_perf_typecode_dist)))\n",
    "\n",
    "    # load the takeoff and landing airport location information to the dataframe. 18s\n",
    "    all_airports = process_airport_list()\n",
    "\n",
    "    # add the estimated departure airport location information to the dataframe\n",
    "    # - this will remove all flights for which the departure airport is unknown\n",
    "    flights_with_perf_typecode_dist_dep = pd.merge(flights_with_perf_typecode_dist, all_airports, how='inner', left_on = 'estdepartureairport', right_on = 'ident')\n",
    "    flights_with_perf_typecode_dist_dep.rename(columns = {'latitude_deg': 'estdeparturelat', 'longitude_deg': 'estdeparturelong', 'elevation_ft': 'estdeparturealt_ft'}, inplace=True)\n",
    "    flights_with_perf_typecode_dist_dep.drop(columns = ['ident', 'gps_code'], inplace=True)\n",
    "\n",
    "    # count the number of flights with no departure airport in the database\n",
    "    print(\"Number of flights with no departure airport location info in database: \" + str(len(flights_with_perf_typecode_dist) - len(flights_with_perf_typecode_dist_dep)))\n",
    "\n",
    "    # add the estimated arrival airport location information to the dataframe\n",
    "    all_flights_filtered = pd.merge(flights_with_perf_typecode_dist_dep, all_airports, how='inner', left_on = 'estarrivalairport', right_on = 'ident')\n",
    "    all_flights_filtered.rename(columns = {'latitude_deg': 'estarrivallat', 'longitude_deg': 'estarrivallong', 'elevation_ft': 'estarrivalalt_ft'}, inplace=True)\n",
    "    all_flights_filtered.drop(columns = ['ident', 'gps_code', 'id_x', 'id_y'], inplace=True)\n",
    "    \n",
    "    # save the result to a pickle file\n",
    "    output_dir_expanded = os.path.expanduser(output_dir)\n",
    "    os.makedirs(output_dir_expanded, exist_ok=True)\n",
    "    filename = os.path.join(output_dir_expanded, f\"aircraftdb/{start_time_simple_loop}_to_{stop_time_simple_loop}_{int(query_limit)}_typecodes_airports_added.pkl\")\n",
    "    all_flights_filtered.to_pickle(filename)\n",
    "\n",
    "    # count the number of flights with no arrival airport in the database\n",
    "    print(\"Number of flights with no arrival airport location info in database: \" + str(len(flights_with_perf_typecode_dist_dep) - len(all_flights_filtered)))\n",
    "    print(\"Number of flights with TOLD, typecode, performance model, distance, and departure/arrival airport info: \" + str(len(all_flights_filtered)))\n",
    "    return filename\n",
    "\n",
    "month_starts = pd.date_range(start=pd.to_datetime(start_time_str), end=pd.to_datetime(stop_time_str), freq='MS', tz='UTC')\n",
    "with Pool(cpu_count() // 2) as pool:\n",
    "    results = pool.map(process_month, month_starts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c2d1c",
   "metadata": {},
   "source": [
    "# part3 but slow, creates stats for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6de9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data from 2023-01-01 00:00:00+00:00 to 2023-01-31 23:59:59+00:00\n",
      "Saved result_df to /scratch/omg28/Data/aircraftdb/2023-01-01_to_2023-01-31_150000_typecodes_added.pkl\n",
      "Saved processed airports to /scratch/omg28/Data/airportdb/processed_airports.csv\n",
      "Processing data from 2023-02-01 00:00:00+00:00 to 2023-02-28 23:59:59+00:00\n",
      "Saved result_df to /scratch/omg28/Data/aircraftdb/2023-02-01_to_2023-02-28_150000_typecodes_added.pkl\n",
      "Saved processed airports to /scratch/omg28/Data/airportdb/processed_airports.csv\n",
      "Processing data from 2023-03-01 00:00:00+00:00 to 2023-03-31 23:59:59+00:00\n",
      "Saved result_df to /scratch/omg28/Data/aircraftdb/2023-03-01_to_2023-03-31_150000_typecodes_added.pkl\n",
      "Saved processed airports to /scratch/omg28/Data/airportdb/processed_airports.csv\n",
      "Processing data from 2023-04-01 00:00:00+00:00 to 2023-04-30 23:59:59+00:00\n",
      "Saved result_df to /scratch/omg28/Data/aircraftdb/2023-04-01_to_2023-04-30_150000_typecodes_added.pkl\n",
      "Saved processed airports to /scratch/omg28/Data/airportdb/processed_airports.csv\n",
      "Processing data from 2023-05-01 00:00:00+00:00 to 2023-05-31 23:59:59+00:00\n",
      "Saved result_df to /scratch/omg28/Data/aircraftdb/2023-05-01_to_2023-05-31_150000_typecodes_added.pkl\n",
      "Saved processed airports to /scratch/omg28/Data/airportdb/processed_airports.csv\n",
      "Processing data from 2023-06-01 00:00:00+00:00 to 2023-06-30 23:59:59+00:00\n",
      "Saved result_df to /scratch/omg28/Data/aircraftdb/2023-06-01_to_2023-06-30_150000_typecodes_added.pkl\n",
      "Saved processed airports to /scratch/omg28/Data/airportdb/processed_airports.csv\n",
      "Processing data from 2023-07-01 00:00:00+00:00 to 2023-07-31 23:59:59+00:00\n",
      "Saved result_df to /scratch/omg28/Data/aircraftdb/2023-07-01_to_2023-07-31_150000_typecodes_added.pkl\n",
      "Saved processed airports to /scratch/omg28/Data/airportdb/processed_airports.csv\n",
      "Processing data from 2023-08-01 00:00:00+00:00 to 2023-08-31 23:59:59+00:00\n",
      "Saved result_df to /scratch/omg28/Data/aircraftdb/2023-08-01_to_2023-08-31_150000_typecodes_added.pkl\n",
      "Saved processed airports to /scratch/omg28/Data/airportdb/processed_airports.csv\n",
      "Processing data from 2023-09-01 00:00:00+00:00 to 2023-09-30 23:59:59+00:00\n",
      "Saved result_df to /scratch/omg28/Data/aircraftdb/2023-09-01_to_2023-09-30_150000_typecodes_added.pkl\n",
      "Saved processed airports to /scratch/omg28/Data/airportdb/processed_airports.csv\n",
      "Processing data from 2023-10-01 00:00:00+00:00 to 2023-10-31 23:59:59+00:00\n",
      "Saved result_df to /scratch/omg28/Data/aircraftdb/2023-10-01_to_2023-10-31_150000_typecodes_added.pkl\n",
      "Saved processed airports to /scratch/omg28/Data/airportdb/processed_airports.csv\n",
      "Processing data from 2023-11-01 00:00:00+00:00 to 2023-11-30 23:59:59+00:00\n",
      "Saved result_df to /scratch/omg28/Data/aircraftdb/2023-11-01_to_2023-11-30_150000_typecodes_added.pkl\n",
      "Saved processed airports to /scratch/omg28/Data/airportdb/processed_airports.csv\n",
      "Processing data from 2023-12-01 00:00:00+00:00 to 2023-12-31 23:59:59+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omg28/ConflictTrails/match_icao_model.py:53: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  flight_df = pd.concat([flight_df, load_saved_fd4(current_day, flight_db_path, query_limit)], axis=0, ignore_index=True)\n",
      "/home/omg28/ConflictTrails/match_icao_model.py:53: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  flight_df = pd.concat([flight_df, load_saved_fd4(current_day, flight_db_path, query_limit)], axis=0, ignore_index=True)\n",
      "/home/omg28/ConflictTrails/match_icao_model.py:53: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  flight_df = pd.concat([flight_df, load_saved_fd4(current_day, flight_db_path, query_limit)], axis=0, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved result_df to /scratch/omg28/Data/aircraftdb/2023-12-01_to_2023-12-31_150000_typecodes_added.pkl\n",
      "Saved processed airports to /scratch/omg28/Data/airportdb/processed_airports.csv\n"
     ]
    }
   ],
   "source": [
    "# process the loaded time period of data. - 12s\n",
    "# - matches 24 bit transponder codes from Feb 2025 aircraft metadata database with the ICAO typecode\n",
    "# - removes all flights for which the transponder code does not have a corresponding ICAO typecode\n",
    "# - removes all flights for which the takeoff OR landing airport is unknown\n",
    "def process_month(start_time_str_loop):\n",
    "    # Set end date to last day of month at 23:59:59 UTC\n",
    "    stop_time_str_loop = (start_time_str_loop + pd.offsets.MonthEnd(1)).replace(hour=23, minute=59, second=59)\n",
    "    start_time_simple_loop = pd.to_datetime(start_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    stop_time_simple_loop = pd.to_datetime(stop_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    print(f\"Processing data from {start_time_str_loop} to {stop_time_str_loop}\")\n",
    "\n",
    "    flights_with_typecode, num_acft_in_metadata, num_acftmetadata_entries_after_filtering, num_all_seen_flights, num_no_arrival_airport, num_no_departure_airport, num_no_airport, num_removed_airports, num_flights_before_icao24_filtering, num_flights_after_icao24_filtering, num_flights_lost_icao_filtering, num_acft_entries_no_matched_flight = match_icao_model(str(start_time_str_loop), str(stop_time_str_loop), query_limit, \n",
    "                                            aircraft_db_path = \"/scratch/omg28/Data/aircraftdb/aircraft-database-complete-2025-02.csv\",\n",
    "                                            flight_db_path = \"/scratch/omg28/Data/no_track2023/\",\n",
    "                                            output_dir = \"/scratch/omg28/Data/aircraftdb/\")\n",
    "    \n",
    "    # Load in the list of ICAO typecodes that have a performance model - 7.5s\n",
    "    performance_model_typecodes = pd.read_pickle('/scratch/omg28/Data/aircraftdb/performance_models_typecodes.pkl')\n",
    "\n",
    "    # inner join the loaded year of data with the scraped aircraft list\n",
    "    flights_with_perf_typecode = pd.merge(flights_with_typecode, performance_model_typecodes, how='inner', on = 'typecode')\n",
    "    \n",
    "    num_flights_after_removing_no_perf_model = len(flights_with_perf_typecode)\n",
    "    num_flights_with_no_perf_model = len(flights_with_typecode) - num_flights_after_removing_no_perf_model\n",
    "\n",
    "    # print a list of typecodes in complete_flights_perf_model that do not have a performance model\n",
    "    num_flight_but_no_typecode = flights_with_typecode[~flights_with_typecode['typecode'].isin(performance_model_typecodes['typecode'])].shape[0]\n",
    "    typecodes_with_flight_but_no_performance_model = flights_with_typecode[~flights_with_typecode['typecode'].isin(performance_model_typecodes['typecode'])]['typecode'].unique()\n",
    "    '''\n",
    "    # print a list of typecodes in performance_model_typecodes that do not correspond to a flight\n",
    "    missing_flights = performance_model_typecodes[~performance_model_typecodes['typecode'].isin(flights_with_typecode['typecode'])]\n",
    "    print(\"Number of performance models with no flights: \" + str(len(missing_flights['typecode'].unique())))\n",
    "    print(\"Number of flights with TOLD, typecode, and performance model: \" + str(len(flights_with_perf_typecode)))\n",
    "    '''\n",
    "    # count all flights starting and ending at the same airport\n",
    "    num_flights_no_dist = len(flights_with_perf_typecode[flights_with_perf_typecode['estdepartureairport'] == flights_with_perf_typecode['estarrivalairport']])\n",
    "\n",
    "    # remove all flights starting or ending at the same airport\n",
    "    flights_with_perf_typecode_dist = flights_with_perf_typecode[flights_with_perf_typecode['estdepartureairport'] != flights_with_perf_typecode['estarrivalairport']]\n",
    "    num_flights_with_perf_typecode_dist = len(flights_with_perf_typecode_dist)\n",
    "\n",
    "    # load the takeoff and landing airport location information to the dataframe. 18s\n",
    "    all_airports = process_airport_list()\n",
    "\n",
    "    # add the estimated departure airport location information to the dataframe\n",
    "    flights_with_perf_typecode_dist_dep = pd.merge(flights_with_perf_typecode_dist, all_airports, how='inner', left_on = 'estdepartureairport', right_on = 'ident')\n",
    "    num_flights_missing_dep_aprt_loc_in_ourflights = len(flights_with_perf_typecode_dist) - len(flights_with_perf_typecode_dist_dep)\n",
    "    flights_with_perf_typecode_dist_dep.rename(columns = {'latitude_deg': 'estdeparturelat', 'longitude_deg': 'estdeparturelong', 'elevation_ft': 'estdeparturealt_ft'}, inplace=True)\n",
    "    flights_with_perf_typecode_dist_dep.drop(columns = ['ident', 'gps_code'], inplace=True)\n",
    "\n",
    "    # add the estimated arrival airport location information to the dataframe\n",
    "    all_flights_filtered = pd.merge(flights_with_perf_typecode_dist_dep, all_airports, how='inner', left_on = 'estarrivalairport', right_on = 'ident')\n",
    "    num_flights_missing_arr_aprt_loc_in_ourflights = len(flights_with_perf_typecode_dist) - len(all_flights_filtered)\n",
    "    all_flights_filtered.rename(columns = {'latitude_deg': 'estarrivallat', 'longitude_deg': 'estarrivallong', 'elevation_ft': 'estarrivalalt_ft'}, inplace=True)\n",
    "    all_flights_filtered.drop(columns = ['ident', 'gps_code', 'id_x', 'id_y'], inplace=True)\n",
    "    \n",
    "    # save the result to a pickle file\n",
    "    output_dir_expanded = os.path.expanduser(output_dir)\n",
    "    os.makedirs(output_dir_expanded, exist_ok=True)\n",
    "    filename = os.path.join(output_dir_expanded, f\"aircraftdb/{start_time_simple_loop}_to_{stop_time_simple_loop}_{int(query_limit)}_typecodes_airports_added.pkl\")\n",
    "    all_flights_filtered.to_pickle(filename)\n",
    "\n",
    "    # count the number of flights with no arrival airport in the database\n",
    "    return filename, num_acft_in_metadata, num_acftmetadata_entries_after_filtering, num_all_seen_flights, num_no_arrival_airport, num_no_departure_airport, num_no_airport, num_removed_airports, num_flights_before_icao24_filtering, num_flights_after_icao24_filtering, num_flights_lost_icao_filtering, num_acft_entries_no_matched_flight, num_flights_after_removing_no_perf_model, typecodes_with_flight_but_no_performance_model, num_flights_with_no_perf_model, num_flight_but_no_typecode, num_flights_no_dist, num_flights_with_perf_typecode_dist, num_flights_missing_dep_aprt_loc_in_ourflights, num_flights_missing_arr_aprt_loc_in_ourflights, num_all_flights_filtered\n",
    "\n",
    "month_starts = pd.date_range(start=pd.to_datetime(start_time_str), end=pd.to_datetime(stop_time_str), freq='MS', tz='UTC')\n",
    "results = []\n",
    "for start_time_str_loop in month_starts: ############# 2 ########################### 3 ####################### 4 #################### 5 ################## 6 ################ 7 ######################### 8 ################################### 9 #################################### 10 ################################### 11 ####################### 12 ############################################# 13 ######################################### 14 ##################### 15 ##################### 16 ####################### 17 ########################################## 18 ################################################ 19 ######################## 20 ####################\n",
    "    result, num_acft_in_metadata, num_acftmetadata_entries_after_filtering, num_all_seen_flights, num_no_arrival_airport, num_no_departure_airport, num_no_airport, num_removed_airports, num_flights_before_icao24_filtering, num_flights_after_icao24_filtering, num_flights_lost_icao_filtering, num_acft_entries_no_matched_flight, num_flights_after_removing_no_perf_model, typecodes_with_flight_but_no_performance_model, num_flights_with_no_perf_model, num_flight_but_no_typecode, num_flights_no_dist, num_flights_with_perf_typecode_dist, num_flights_missing_dep_aprt_loc_in_ourflights, num_flights_missing_arr_aprt_loc_in_ourflights, num_all_flights_filtered = process_month(start_time_str_loop)\n",
    "    results.append((result, num_acft_in_metadata, num_acftmetadata_entries_after_filtering, num_all_seen_flights, num_no_arrival_airport, num_no_departure_airport, num_no_airport, num_removed_airports, num_flights_before_icao24_filtering, num_flights_after_icao24_filtering, num_flights_lost_icao_filtering, num_acft_entries_no_matched_flight, num_flights_after_removing_no_perf_model, typecodes_with_flight_but_no_performance_model, num_flights_with_no_perf_model, num_flight_but_no_typecode, num_flights_no_dist, num_flights_with_perf_typecode_dist, num_flights_missing_dep_aprt_loc_in_ourflights, num_flights_missing_arr_aprt_loc_in_ourflights, num_all_flights_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d12dafc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6478740  1 # aircraft in the metadata database\n",
      "5285076  2 # aircraft metadata entries after removing blank typecodes\n",
      "39598095  3 # all flights seen by opensky in 2023 before any processing\n",
      "4643745  4 # flights with empty arrival airport in the opensky flight list, but departure airport in the opensky flight list\n",
      "6888654  5 # flights with empty departure airport in the opensky flight list, but arrival airport in the opensky flight list\n",
      "1735259  6 # flights with no arrival and no departure in the opensky flight list\n",
      "13267658  7 # flights removed from the opensky flight list due to any missing airport info, sum of 3,4,5\n",
      "26330437  8 # flights with arrival and departure airport, before matching ICAO24 code to an entry in the aircraft metadata database, 3-7\n",
      "23748899  9 # flights with arrival and departure airport and ICAO24 code in the metadata database with a corresponding ICAO typecode\n",
      "2581538  10 # flights with arrival and departure airport but no metadata database ICAO24 code match- number is ignored, 8-9\n",
      "4035257  11 # aircraft metadata database entries with no matching flight in the opensky flight list\n",
      "19803773  12 # all flights with arrival and departure airport, ICAO24 code in the metadata database with a corresponding ICAO typecode, and performance model for its ICAO typecode\n",
      "          13 list of type designators in the metadata database with flight but no performance model- good target for default perf model. number of unique typecodes,  1143 , unique typecodes,  ['KR30', 'UF13', 'DAKH', 'BL8', 'R300', \"'42.N342'\", 'SGUP', 'SLG2', 'WT10', 'DH82', \"'215'\", 'PTSS', 'CH80', 'H160', \"'3 E 42'\", 'AT76', 'PISI', 'L8', 'PA20', 'STRE', 'KL07', 'AT8T', 'AP20', \"'4-253'\", \"'8-56S15'\", 'T33', 'MR35', 'OSCR', 'B25', 'G-215', 'C120', 'F8L', \"'31T-8004044'\", 'PACE', 'KP2', 'MC45', 'TAYA', \"'22-72481'\", 'TAGO', 'C551', 'SF25', 'S211', 'AS25', 'M108', 'FURY', 'J2', 'TOUR', 'FK14', 'C205', 'FOX', 'LJ24', 'F5EX', 'GY20', 'C185', 'RV10', 'CTAH', 'YK54', 'TAIL', 'VNTE', 'E75S', 'C501', 'CH7', 'SF2', 'BB-1222', \"'1699'\", 'A1', 'CH60', 'LAA 376-15200', \"'4636336'\", 'SPIT', 'G450', 'A119', 'VTUR', 'B24', 'RYST', 'H207', 'RS21', 'M55', 'BU33', 'SP7', 'PUP', 'CH7A', 'H53', 'EXPL', 'NG4', 'COLT', 'MS76', 'BR23', 'BB-305', 'BN2P', 'SIRA', 'D253', 'MOR2', \"'31072'\", 'NIPR', \"'10899'\", 'KA27', 'M28', 'LN27', 'T18', 'NNJA', 'CAMP', 'BE18', 'DH8B', 'DG50', 'P210', 'SWAK', 'E290', 'ERCO', \"'3324'\", \"'354'\", 'AS65', \"'680A Citation Latitude'\", 'ALH', 'AS31', 'RS12', 'FB1A', \"'32'\", 'GDUK', 'PETR', 'S58P', 'CHR4', 'LGEZ', 'S108', \"'4203'\", 'P750', \"'31-8012091'\", 'M7T', 'GABR', 'COL3', 'STCH', \"'2331'\", 'SA75', 'A32P', 'S58T', 'PIPA', 'CL61', 'ONEX', \"'25-1155'\", 'SM19', \"'1799'\", 'B609', 'M7', \"'739'\", \"'525C0140'\", 'B58T', \"'85031'\", 'BPAT', 'R66', 'ELSP', 'B17', 'J4', 'DH2T', 'BB883', 'SBOY', 'BB 324', 'B407', 'TEX2', 'VENT', 'SF50', 'CJ6', \"'9397'\", 'MUS2', 'DFLY', \"'13032'\", 'CB1', 'DRON', 'J1', 'SUBA', 'SGRA', 'DR10', 'H500', 'BROU', \"'90/118'\", 'M4', \"'69E52'\", 'BB1305', 'TFUN', 'H2', 'P92', 'FK9', 'BUSH', 'EPIC', 'ALIG', \"'28-8011056'\", 'G73', 'L18', 'LNC4', 'ALGR', \"'145/73'\", 'H269', \"'0069'\", 'TAYD', 'ALPI', 'GY80', 'GA5C', 'CH20', 'LAA 323-15093', 'T34P', 'LJ70', \"'4173'\", \"'509'\", 'B14A', 'GL7T', 'COUG', 'S05R', \"'28R-7635435'\", 'Q9', \"'966'\", \"'231'\", 'LK19', 'PL4', 'TLEG', 'TNAV', 'AVID', 'SEAW', 'PRIM', 'S22T', 'FBA2', \"'0728'\", 'B26', 'G2GL', 'PIVI', 'AT46', 'P28S', 'CRER', 'AS20', 'PULS', 'LESP', \"'003'\", 'PA25', 'MT', 'VAMP', \"'73965'\", 'JK05', 'TBEE', 'HUNT', 'MAGC', \"'50500453'\", \"'22-7525'\", 'Sr22', 'DG80', 'ACR2', 'PITA', 'FOUG', 'SAVG', \"'40E38'\", 'JAB4', \"'5696'\", 'AS55', 'C68A', 'XL2', 'ZEPH', 'C336', 'T51', 'SB7', 'S61R', 'TL20', \"'374'\", 'CNBR', 'TB21', 'BK17', 'HR10', 'GA8', \"'83518'\", 'SKYR', \"'2842214'\", 'RV8', \"'25-0554'\", 'P4Y', 'EVOT', 'CL2P', 'G46', 'BR61', 'SLG4', \"'344'\", 'FNKB', 'LGND', \"'1309'\", 'M22', 'Z37P', 'FM25', 'AS02', 'J3', 'AUS6', 'BE17', 'DISC', \"'46428'\", 'SHIP', 'BER2', 'C700', \"'182-81060'\", \"'802-0454'\", 'RC3', 'C10T', 'G550', \"'85054'\", 'CLBR', 'ZA6', \"'120/70'\", 'E737', 'BALL', 'FDCT', 'ST30', 'BRAV', 'BOLT', 'CDUS', 'G109', 'L29', \"'180/115'\", 'STOR', 'PITE', 'B721', 'AS35', 'WA40', 'SBD', 'RS20', 'SBR1', 'SLH4', 'M10', \"'46402'\", \"'1198'\", 'DIMO', 'KC2', 'P337', 'A20', \"'8893'\", 'D11', 'PO2', 'SD4', 'U2', 'D250', 'A189', 'A148', 'SNS7', 'AT5T', \"'85026'\", \"'19'\", 'G12T', 'GAZL', 'TVLB', 'B29', 'B214', 'YK52', 'C77R', \"'208B0727'\", 'PC24', 'C335', 'T20608276', 'HUCO', 'EGRT', 'C123', 'STAR', 'GB1', 'DEFI', 'TRAL', 'PA15', 'DHC4', 'WACC', 'YK11', 'HB21', 'SASP', 'XA42', \"'3381'\", \"'85027'\", \"'057'\", 'AC90', 'PA12', 'C46', 'MXS', \"'28-7890230'\", 'CP30', 'R200', 'A169', 'HR20', 'C5M', 'ALTO', 'JAB2', 'VM1', 'PTS2', 'LJ-1740', 'WW24', \"'85056'\", \"'0759'\", 'CUB2', 'PZ06', 'GLF6', 'AR15', 'SKAR', 'ACAM', 'V10', 'MA5', 'FA8X', 'TEXA', \"'2019'\", 'L12', 'A139', 'SKRA', 'NSTR', 'BE24', 'N3N', 'ESCP', 'LA8', 'UHEL', 'P68T', 'LAMA', 'B427', 'SONX', 'J5', \"'0294'\", 'EL10', \"'46443'\", 'T206', 'HROC', 'FDF2', 'T34T', \"'116'\", 'CNUK', 'P68C', 'LK17', 'G1', 'DHC3', 'GOLF', 'B209', \"'3388'\", \"'802-0614'\", 'R22', 'VO10', 'A660', \"'8-117B46'\", 'AJ27', 'TH-1129', 'CH70', \"'020'\", 'AS28', \"'085 CS'\", 'MCR4', 'G164', 'C188', 'BW6T', \"'31301'\", 'M8', \"'7093'\", \"'42.N337'\", \"'243'\", 'EAGL', 'HN70', \"'46475'\", 'BD5', 'P51', 'SATA', 'PA17', 'JUNR', \"'174'\", 'BE65', 'BREZ', 'S51D', \"'25-0389'\", 'CVLP', 'B429', 'PA22', 'EC25', 'LANC', 'ARES', 'AR6T', 'F260', 'PA28A', 'VELT', 'MC23', 'SX30', 'MP02', 'CH65', 'DV2', 'AUJ2', 'HCAT', 'B779', 'WAIX', 'CP10', 'PNR4', 'G400', 'KODI', 'K900', 'EN48', 'C240', 'HRON', 'AS29', 'S3', 'FA62', 'EAGX', 'BEAR', \"'8-138S31'\", 'QIC2', 'T20608787', 'UH12', 'FAET', 'LNC2', 'SU29', 'G2T1', 'D140', 'C295', 'FU24', 'YK55', 'P38', 'VNOM', 'H125', 'RF5', 'S12S', 'RODS', 'G21', \"'3391'\", \"'46-22051'\", 'EFOX', 'C510', 'GLST', \"'2698'\", 'EVOP', 'DUOD', 'DC3', 'CA4', \"'1154'\", 'DH83', \"'822'\", 'RV9', 'CLON', 'KA32', 'M200', 'C72R', 'EV55', 'V22', 'CH7B', 'VNTR', 'WB57', 'GA7', 'PAY1', \"'061'\", 'KFIR', 'GLF2', 'SA30', 'CH30', 'VR7', 'C82T', 'LJ75', 'BX2', 'SSTL', 'SAKO', \"'560-0304'\", 'ALSL', 'S6', 'GA8C', 'UE 368', 'J400', 'SB91', \"'8-64A38'\", \"'85029'\", 'AA1', 'MITE', \"'1103'\", 'AAT4', 'SREY', 'CC11', \"'3387'\", 'DG40', 'VALI', \"'232'\", 'TARR', 'F86', \"'363'\", 'TS11', 'CJ1', 'L13', 'C55B', 'AS26', 'SHEK', 'KFAB', \"'3269'\", 'TB20', 'DR22', 'K100', 'VEZE', 'UE359', 'PIAT', 'AS3B', \"'3 E 318'\", 'C195', 'VTRA', 'BB 535', 'M326', 'DHC-6-300', 'PPRO', 'PNTH', \"'26033'\", 'DH89', 'MG17', 'TOXO', 'Z43', 'BKUT', 'F2LX', 'LNP4', 'PA30', \"'28-7725121'\", 'RV12', 'J300', 'B200', 'POLI', \"'46404'\", 'AT3P', 'PC21', \"'11207'\", 'P66P', 'MX80', 'JSX', 'L11', 'BCAT', 'WIRR', \"'2092'\", 'M6', 'BE30', \"'3257050'\", 'L5', 'RF10', 'KIS4', 'SU31', 'E550', 'G70', 'T28', 'L90', 'S330', 'E230', 'RELI', 'SU26', 'MD60', 'EXNG', 'GC1', \"'31-8052048'\", 'PARA', 'CP21', 'YK18', \"'3609'\", \"'049'\", 'CC19', 'TF19', 'E530', \"'31125'\", \"'34-7450076'\", 'PAT4', 'A149', 'AT6T', 'PZ4M', 'XA41', 'MM22', \"'8-234B148'\", \"'3339'\", 'TRDO', 'E75L', 'S92', 'P40', \"'761'\", 'GA6C', 'EV97', \"'8-82B19'\", 'P47', \"'40.1128'\", \"'1039'\", \"'1581'\", 'S52', 'ADVE', 'DA62', \"'763'\", 'PA24', 'STLN', 'RBEL', 'E314', \"'3'\", 'AC80', \"'1258'\", 'SABW', 'P136', 'S2P', 'ISPT', 'AS50', 'AP32', \"'002'\", 'E390', 'TUCR', \"'31017'\", 'MD52', 'MF17', 'JD2', \"'21059123'\", 'WACD', 'PIAE', \"'24-3251'\", 'A33P', \"'0573'\", 'PELI', 'BE19', 'RISN', 'PC6P', 'GA7C', \"'3352'\", 'CRES', 'GYRO', 'PEGZ', 'Z42', 'RC70', 'NIMB', 'PA16', 'NAVI', \"'510-0341'\", 'P8', 'SKIM', 'ELTO', 'BE77', \"'211'\", 'ZZZZ', 'JFOX', 'SU57', 'S10S', \"'3274'\", 'BN2B', 'GLF', 'TL30', 'CL64', 'DV1', 'DC3T', 'LA60', 'TF21', 'DA20', 'PZ3T', 'CLA', \"'750'\", 'YK50', \"'28-8290082'\", 'CAT', 'KA 11038186', 'DHC1', 'CH2T', 'HB23', 'JS3E', 'S64', 'B787', 'SC01', \"'5E 216X56'\", 'CASS', 'JS1J', \"'784 SLCH 912'\", 'UE75', 'L200', 'S355', 'Y18T', 'U21', 'T6', 'SVNH', 'BB-879', 'P06T', 'PA11', 'C320', 'T35', 'ECHO', \"'15'\", 'B36T', 'PNR3', 'PTS1', \"'40.N324'\", 'COY2', 'E295', 'CORS', 'P208', 'LEG2', \"'80385'\", 'LJ-1744', \"'24-0262'\", 'PT22', 'ONE', 'G500', 'DG60', 'TWST', 'G600', 'TRIM', 'DH60', 'P149', \"'46431'\", 'D1', 'MC01', 'RV3', 'PNR2', 'SS2P', 'A600', 'F156', 'DHC2', \"'56287'\", 'M110', 'FW90', 'A210', 'TR55', 'ASTO', 'C190', 'DO27', 'C408', 'F9EX', 'P220', \"'85053'\", \"'8-170 B 94'\", 'COUR', \"'690'\", 'C175', 'SA10', 'SYMP', 'D18', 'CORO', 'T2', 'ALO3', 'MCR1', 'CHR1', 'MI24', \"'670313'\", 'C170', 'C82S', 'E545', 'CRUZ', 'UAV', 'KP5', 'LA25', 'PP3', \"'333'\", 'IMPU', \"'22-72479'\", 'LAA 323-14860', \"'4636205'\", \"'28-7916344'\", \"'2844089'\", 'C411', 'L40', 'WT9', 'G200', 'BE4W', 'LNCE', 'BU31', 'TEST', \"'028'\", \"'0000'\", 'WCAT', \"'209'\", \"'0900'\", 'PA36', \"'31126'\", 'ALO2', 'VL3', 'AT73', 'A380', 'LJ-1750', 'WA41', 'COZY', 'PL2', 'F2EX', 'LA4', 'CUCA', 'PTMS', \"'82945'\", 'TIGR', \"'511'\", 'ST75', 'RV6', 'XNOS', 'P28B', 'STRM', 'MX1T', 'TWEN', 'A223', 'LJ-1746', \"'6317'\", \"'046'\", 'WACE', \"'26204'\", 'EVSS', 'MM16', \"'227'\", 'DO28', 'GLSP', 'DT45', 'FUSI', 'COL4', 'EUPA', 'EXPR', 'AT42', 'MX2', 'TBM', \"'5165'\", 'P28U', \"'377'\", 'EC75', 'VJ22', 'MOSQ', 'EC45', 'VELO', \"'614'\", \"'24-1039'\", 'CH75', 'GA20', 'A330', 'B222', 'EN28', \"'3382'\", 'AIGT', \"'183'\", 'WHIL', 'C162', 'PA14', \"'85033'\", 'TRIS', 'CH64', 'ARCE', 'B505', 'PK23', \"'282'\", 'AP22', 'E6', 'ASO4', 'A500', 'KTOO', 'DC4', 'MIMU', 'RW19', \"'310R0872'\", 'G44', 'M9', 'P32T', 'HIGH', 'SRPRV7', 'C140', \"'46451'\", 'T50', 'WISP', 'FH11', 'SASY', 'MAGN', 'G650', \"'134'\", 'O1', \"'3390'\", 'TEAL', 'KFAS', \"'2136'\", 'RV7', 'S15S', 'HA4T', 'DG1T', 'L37', 'JPRO', 'EA50', 'H750', \"'00'\", 'V500', 'LS10', 'EDGT', \"'85055'\", 'LJ-1749', \"'404'\", 'PINO', 'AVTR', 'E200', 'BL17', 'KMAX', \"'441-0216'\", 'CL65', 'TTRS', 'AUS5', 'P68', \"'1C.MD101'\", 'RV4', 'WA50', \"'2843111'\", \"'109'\", 'GLAS', 'NOMA', 'RLU1', 'WACF', 'U16', \"'26088'\", 'SSAB', 'DR30', 'SS2T', 'BE35', 'TRF1', 'GLID', \"'2010-03-05'\", 'WA42', 'IR23', \"'71'\", 'LCA', \"'1389'\", 'B105', 'BD4', 'G202', 'CP60', 'ARCP', 'LS8', \"'28-21528'\", \"'29-0509'\", 'PUSH', \"'238'\", 'H900', 'TMUS', 'G73T', 'YK12', 'GP3', 'YALE', \"'8-118B47'\", 'SJ30', 'BB1224', 'UE-395', 'T210', \"'172S'\", 'J600', 'GROU', 'LJ-1747', \"'0012'\", 'E300', \"'46400'\", 'Z26', 'BE56', 'GL6T', 'UNDEFINED', 'C500', 'KT1', 'CT4', 'EC55', \"'0066'\", 'ACRO', \"'3325'\", 'UE333', 'BRB2', 'P212', 'TBM9', 'NG5', 'PIVE', \"'340A0096'\", 'BT36', 'YAK3', 'DA2', 'TERR', 'ACAR', 'CA65', \"'32-40064'\", 'AR11', 'CVLT', 'C27J', 'SRAI', 'M5', \"'100-0195'\", \"'11AC-1611'\", 'RV14', 'E400', 'MM24', 'AS24', \"'17271531'\", 'FFLO', \"'6309'\", \"'8064'\", 'AA5', 'HURI', 'B212', 'HUSK', 'GX', 'LYNX', 'G250', \"'85034'\", \"'90/165'\", \"'58E48'\", 'KR2', 'EK30', \"'332'\", 'ELA7', 'RAF2', \"'46447'\", 'C180', 'BE9T', 'DG15', 'BT7', 'AS21', 'H53S', 'MU30', 'CA25', 'G103', \"'85032'\", 'B23', 'EC20', 'BB-752', 'B47G', 'SURN', 'DV20', 'BB-455', \"'25626'\", 'LJ-1745', 'S10', 'P63', 'AAT3', \"'73963'\", 'E35L', 'M346', 'DC6', \"'120/73'\", 'BABY', 'A37', 'A5', 'EXEC', 'C42', 'JABI', 'VUT1', 'MCOY', 'ULAC', \"'28-8190083'\", 'S55P', 'S401', 'BB-1302', \"'177RG1335'\", 'EC30', \"'1951'\", 'RF6', \"'85028'\", nan, 'UF10', 'S1', 'E45X', 'EDGE', 'VP2', \"'1508'\", 'SWOR', \"'234'\", 'MG15', 'H145', 'SMAX', 'SHRK', 'FA24', 'SA02', 'TAYB', \"'395'\", 'AT3T', 'G2CA', 'WACO', \"'85030'\", 'G120', 'H46']\n",
      "3945126  14 # all flights with arrival and departure airport, ICAO24 code in the metadata database with a corresponding ICAO typecode, and no performance model\n",
      "3945126  15 # of all flights with list of all unique type designators with arrival and departure airport, ICAO24 code in the metadata database with a corresponding ICAO `typecode, but no performance model. duplicate of 14\n",
      "2498775  16 10, but with same departure and arrival airport (no distance travelled)\n",
      "17304998  17 10 but with different departure and arrival airports\n",
      "278081  18 15 and departure airport has location in our airports database\n",
      "670938  19 15 but departure airport has NO location in our airports database\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/traffic/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:218\u001b[39m, in \u001b[36m_na_arithmetic_op\u001b[39m\u001b[34m(left, right, op, is_cmp)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/traffic/lib/python3.12/site-packages/pandas/core/computation/expressions.py:242\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(op, a, b, use_numexpr)\u001b[39m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[32m    241\u001b[39m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/traffic/lib/python3.12/site-packages/pandas/core/computation/expressions.py:131\u001b[39m, in \u001b[36m_evaluate_numexpr\u001b[39m\u001b[34m(op, op_str, a, b)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     result = \u001b[43m_evaluate_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/traffic/lib/python3.12/site-packages/pandas/core/computation/expressions.py:73\u001b[39m, in \u001b[36m_evaluate_standard\u001b[39m\u001b[34m(op, op_str, a, b)\u001b[39m\n\u001b[32m     72\u001b[39m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/traffic/lib/python3.12/site-packages/pandas/core/roperator.py:11\u001b[39m, in \u001b[36mradd\u001b[39m\u001b[34m(left, right)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mradd\u001b[39m(left, right):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mright\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for +: 'int' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(results[i][\u001b[32m18\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(results))), \u001b[33m\"\u001b[39m\u001b[33m 18 15 and departure airport has location in our airports database\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(results[i][\u001b[32m19\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(results))), \u001b[33m\"\u001b[39m\u001b[33m 19 15 but departure airport has NO location in our airports database\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33m 20 all flights with TOLD, distance, ICAO24 match to type designator, and performance model for its ICAO typecode\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/traffic/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/traffic/lib/python3.12/site-packages/pandas/core/arraylike.py:190\u001b[39m, in \u001b[36mOpsMixin.__radd__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__radd__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__radd__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mradd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/traffic/lib/python3.12/site-packages/pandas/core/frame.py:7913\u001b[39m, in \u001b[36mDataFrame._arith_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   7910\u001b[39m \u001b[38;5;28mself\u001b[39m, other = \u001b[38;5;28mself\u001b[39m._align_for_op(other, axis, flex=\u001b[38;5;28;01mTrue\u001b[39;00m, level=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   7912\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(\u001b[38;5;28mall\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m7913\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dispatch_frame_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7914\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(new_data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/traffic/lib/python3.12/site-packages/pandas/core/frame.py:7945\u001b[39m, in \u001b[36mDataFrame._dispatch_frame_op\u001b[39m\u001b[34m(self, right, func, axis)\u001b[39m\n\u001b[32m   7942\u001b[39m right = lib.item_from_zerodim(right)\n\u001b[32m   7943\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(right):\n\u001b[32m   7944\u001b[39m     \u001b[38;5;66;03m# i.e. scalar, faster than checking np.ndim(right) == 0\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m7945\u001b[39m     bm = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7946\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(bm, axes=bm.axes)\n\u001b[32m   7948\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(right, DataFrame):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/traffic/lib/python3.12/site-packages/pandas/core/internals/managers.py:361\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m             kwargs[k] = obj[b.mgr_locs.indexer]\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(f):\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     applied = \u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    363\u001b[39m     applied = \u001b[38;5;28mgetattr\u001b[39m(b, f)(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/traffic/lib/python3.12/site-packages/pandas/core/internals/blocks.py:393\u001b[39m, in \u001b[36mBlock.apply\u001b[39m\u001b[34m(self, func, **kwargs)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, **kwargs) -> \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[32m    389\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    390\u001b[39m \u001b[33;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[32m    391\u001b[39m \u001b[33;03m    one\u001b[39;00m\n\u001b[32m    392\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m     result = maybe_coerce_values(result)\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._split_op_result(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/traffic/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:283\u001b[39m, in \u001b[36marithmetic_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    279\u001b[39m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    281\u001b[39m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[32m    282\u001b[39m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     res_values = \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/traffic/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:227\u001b[39m, in \u001b[36m_na_arithmetic_op\u001b[39m\u001b[34m(left, right, op, is_cmp)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    221\u001b[39m         left.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mobject\u001b[39m\n\u001b[32m    222\u001b[39m     ):\n\u001b[32m   (...)\u001b[39m\u001b[32m    225\u001b[39m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[32m    226\u001b[39m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m         result = \u001b[43m_masked_arith_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/traffic/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:182\u001b[39m, in \u001b[36m_masked_arith_op\u001b[39m\u001b[34m(x, y, op)\u001b[39m\n\u001b[32m    179\u001b[39m         mask = np.where(y == \u001b[32m1\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, mask)\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m         result[mask] = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxrav\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m np.putmask(result, ~mask, np.nan)\n\u001b[32m    185\u001b[39m result = result.reshape(x.shape)  \u001b[38;5;66;03m# 2D compat\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/traffic/lib/python3.12/site-packages/pandas/core/roperator.py:11\u001b[39m, in \u001b[36mradd\u001b[39m\u001b[34m(left, right)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mradd\u001b[39m(left, right):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mright\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "print(sum(results[i][1] for i in range(len(results))), \" 1 # aircraft in the metadata database\")\n",
    "print(sum(results[i][2] for i in range(len(results))), \" 2 # aircraft metadata entries after removing blank typecodes\") # 2-1 is number of blank typecodes in the metadata database\n",
    "print(sum(results[i][3] for i in range(len(results))), \" 3 # all flights seen by opensky in 2023 before any processing\")\n",
    "print(sum(results[i][4] for i in range(len(results))), \" 4 # flights with empty arrival airport in the opensky flight list, but departure airport in the opensky flight list\")\n",
    "print(sum(results[i][5] for i in range(len(results))), \" 5 # flights with empty departure airport in the opensky flight list, but arrival airport in the opensky flight list\")\n",
    "print(sum(results[i][6] for i in range(len(results))), \" 6 # flights with no arrival and no departure in the opensky flight list\")\n",
    "print(sum(results[i][7] for i in range(len(results))), \" 7 # flights removed from the opensky flight list due to any missing airport info, sum of 3,4,5\") \n",
    "print(sum(results[i][8] for i in range(len(results))), \" 8 # flights with arrival and departure airport, before matching ICAO24 code to an entry in the aircraft metadata database, 3-7\") # checked\n",
    "print(sum(results[i][9] for i in range(len(results))), \" 9 # flights with arrival and departure airport and ICAO24 code in the metadata database with a corresponding ICAO typecode\")\n",
    "print(sum(results[i][10] for i in range(len(results))), \" 10 # flights with arrival and departure airport but no metadata database ICAO24 code match- number is ignored, 8-9\") # checked\n",
    "print(sum(results[i][11] for i in range(len(results))), \" 11 # aircraft metadata database entries with no matching flight in the opensky flight list\")\n",
    "print(sum(results[i][12] for i in range(len(results))), \" 12 # all flights with arrival and departure airport, ICAO24 code in the metadata database with a corresponding ICAO typecode, and performance model for its ICAO typecode\")\n",
    "# Flatten all arrays and get unique values for typecodes with flight but no performance model\n",
    "all_typecodes = []\n",
    "for i in range(len(results)):\n",
    "\tall_typecodes.extend(results[i][13])\n",
    "unique_typecodes = list(set(all_typecodes))\n",
    "print(\"          13 list of type designators in the metadata database with flight but no performance model- good target for default perf model. number of unique typecodes, \", len(unique_typecodes), \", unique typecodes, \", unique_typecodes)\n",
    "print(sum(results[i][14] for i in range(len(results))), \" 14 # all flights with arrival and departure airport, ICAO24 code in the metadata database with a corresponding ICAO typecode, and no performance model\")\n",
    "print(sum(results[i][15] for i in range(len(results))), \" 15 # of all flights with list of all unique type designators with arrival and departure airport, ICAO24 code in the metadata database with a corresponding ICAO `typecode, but no performance model. duplicate of 14\")\n",
    "print(sum(results[i][16] for i in range(len(results))), \" 16 10, but with same departure and arrival airport (no distance travelled)\")\n",
    "print(sum(results[i][17] for i in range(len(results))), \" 17 10 but with different departure and arrival airports\")\n",
    "print(sum(results[i][18] for i in range(len(results))), \" 18 15 and departure airport has location in our airports database\")\n",
    "print(sum(results[i][19] for i in range(len(results))), \" 19 15 but departure airport has NO location in our airports database\")\n",
    "print(sum(results[i][20] for i in range(len(results))), \" 20 all flights with TOLD, distance, ICAO24 match to type designator, and performance model for its ICAO typecode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8a321c",
   "metadata": {},
   "source": [
    "# Section 4. Using the great circle distance, calculate the distance between the airport takeoff and landing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd02fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the great circle distance between the departure and arrival airports - 40s\n",
    "\n",
    "# Ensure max_distances_all is defined before the loop\n",
    "max_distances_all = pd.DataFrame(columns=['typecode', 'max_distance_km'])\n",
    "\n",
    "def process_month_distance(args):\n",
    "    start_time_str_loop, stop_time_str_loop, query_limit, output_dir = args\n",
    "    start_time_simple_loop = pd.to_datetime(start_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    stop_time_simple_loop = pd.to_datetime(stop_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    all_flights_filtered = pd.read_pickle(f'/scratch/omg28/Data/aircraftdb/{start_time_simple_loop}_to_{stop_time_simple_loop}_{int(query_limit)}_typecodes_airports_added.pkl')\n",
    "    all_flights_filtered['gc_km'] = all_flights_filtered.apply(\n",
    "        lambda x: distance.great_circle((x['estdeparturelat'], x['estdeparturelong']), (x['estarrivallat'], x['estarrivallong'])).km, axis=1)\n",
    "    all_flights_filtered['gc_FEAT_km'] = all_flights_filtered.apply(lambda x: 1.0387 * x['gc_km'] + 40.5, axis=1)\n",
    "    pd.to_pickle(all_flights_filtered, f'{output_dir}/{start_time_simple_loop}_to_{stop_time_simple_loop}_filtered.pkl')\n",
    "    all_flights_filtered = all_flights_filtered[all_flights_filtered['gc_FEAT_km'] > 200]\n",
    "    print(\"Number of flights with TOLD, typecode, performance model, distance, and departure/arrival airport info over 200 km: \" + str(len(all_flights_filtered)))\n",
    "    max_distances_month = all_flights_filtered.groupby('typecode')['gc_FEAT_km'].max().reset_index()\n",
    "    max_distances_month.columns = ['typecode', 'max_distance_km']\n",
    "    \n",
    "    \n",
    "    max_distances_month.to_csv(f'{output_dir}/aircraftdb/{start_time_simple_loop}_to_{stop_time_simple_loop}_typecode_max_distances.csv', index=False)\n",
    "    return max_distances_month\n",
    "\n",
    "# Prepare arguments for each month\n",
    "month_args = []\n",
    "for start_time_str_loop in pd.date_range(start=pd.to_datetime(start_time_str), end=pd.to_datetime(stop_time_str), freq='MS', tz='UTC'):\n",
    "    stop_time_str_loop = (start_time_str_loop + pd.offsets.MonthEnd(1)).replace(hour=23, minute=59, second=59)\n",
    "    month_args.append((start_time_str_loop, stop_time_str_loop, query_limit, output_dir))\n",
    "\n",
    "with Pool(cpu_count() // 2) as pool:\n",
    "    max_distances_list = pool.map(process_month_distance, month_args)\n",
    "\n",
    "# Merge all max_distances_month DataFrames and keep the maximum for each typecode\n",
    "if max_distances_list:\n",
    "    max_distances_all = pd.concat(max_distances_list).groupby('typecode', as_index=False)['max_distance_km'].max()\n",
    "    # Save the merged max distances dataframe to a .csv file (for the whole period)\n",
    "    max_distances_all.to_csv(f'{output_dir}/aircraftdb/{start_time_simple}_to_{stop_time_simple}_typecode_max_distances.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3518da",
   "metadata": {},
   "source": [
    "# Section 4.5: Process all flights to determine if their GC route is affected by conflict or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ce0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CODE WILL EXECUTE UPON IMPORTING THIS MODULE. CONSIDER CHANGING IN LATER VERSIONS.\n",
    "# FIXME: Decide whether to run this as a .py or in the ipynb notebook.\n",
    "\n",
    "# from label_conflict import label_conflict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066704c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting testcode\n",
    "''''''\n",
    "# plotting the flight paths with conflict zones\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import LineString, Point\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "from geopy import distance\n",
    "from geographiclib.geodesic import Geodesic\n",
    "\n",
    "testerrrrr = pd.read_pickle('/scratch/omg28/Data/2023-01-01_to_2023-01-31_labeled.pkl')[0:1]\n",
    "\n",
    "def create_geodesic_line(lon1, lat1, lon2, lat2, num_points=100):\n",
    "    \"\"\"Create a geodesic line between two points using WGS84 ellipsoid\"\"\"\n",
    "    geod = Geodesic.WGS84\n",
    "    line = geod.InverseLine(lat1, lon1, lat2, lon2)\n",
    "    \n",
    "    # Generate points along the geodesic\n",
    "    points = []\n",
    "    for i in range(num_points):\n",
    "        s = (i / (num_points - 1)) * line.s13\n",
    "        g = line.Position(s)\n",
    "        points.append((g['lon2'], g['lat2']))\n",
    "    \n",
    "    return points\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Load world boundaries\n",
    "try:\n",
    "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "except (AttributeError, KeyError):\n",
    "    world = gpd.read_file('ne_110m_admin_0_countries.zip')\n",
    "\n",
    "# Plot world map as background\n",
    "world.plot(ax=ax, color='lightgray', edgecolor='white', alpha=0.7)\n",
    "\n",
    "# Create conflict zone masks\n",
    "name_col = 'NAME' if 'NAME' in world.columns else 'name'\n",
    "conflict_zones = world[world[name_col].isin(conflict_countries)]\n",
    "\n",
    "# Create 1 degree buffers for conflict countries\n",
    "conflict_buffered = conflict_zones.geometry.buffer(1.0)\n",
    "\n",
    "# Plot conflict zones with transparency\n",
    "conflict_buffered.plot(ax=ax, color='red', alpha=0.3, label='Conflict Zones (1° Buffer)')\n",
    "\n",
    "# Plot flight paths with geodesic routes\n",
    "for idx, flight in testerrrrr.iterrows():\n",
    "    geodesic_points = create_geodesic_line(\n",
    "        flight['estdeparturelong'], flight['estdeparturelat'],\n",
    "        flight['estarrivallong'], flight['estarrivallat']\n",
    "    )\n",
    "    points = np.array(geodesic_points)\n",
    "    lons = points[:, 0]\n",
    "    lats = points[:, 1]\n",
    "\n",
    "    # Find where the longitude jumps more than 180 degrees (dateline crossing)\n",
    "    lon_diff = np.abs(np.diff(lons))\n",
    "    split_indices = np.where(lon_diff > 180)[0] + 1\n",
    "\n",
    "    # Split the points at the dateline\n",
    "    segments = np.split(points, split_indices)\n",
    "\n",
    "    color = 'orange' if flight['crosses_conflict'] else 'blue'\n",
    "    alpha = 0.8 if flight['crosses_conflict'] else 0.3\n",
    "\n",
    "    for seg in segments:\n",
    "        # Only plot if segment has more than 1 point\n",
    "        if len(seg) < 2:\n",
    "            continue\n",
    "        # Do not shift longitudes; just plot as-is to avoid horizontal lines\n",
    "        line = LineString(seg)\n",
    "        gdf_line = gpd.GeoDataFrame([1], geometry=[line])\n",
    "        gdf_line.plot(ax=ax, color=color, alpha=alpha, linewidth=0.5)\n",
    "\n",
    "# Add labels and formatting\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('Geodesic Flight Paths with Conflict Zone Analysis')\n",
    "\n",
    "# Create custom legend\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='blue', alpha=0.3, label='Normal Flights'),\n",
    "    Line2D([0], [0], color='orange', alpha=0.8, label='Conflict-Affected Flights'),\n",
    "    plt.Rectangle((0, 0), 1, 1, facecolor='red', alpha=0.3, label='Conflict Zones (1° Buffer)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower left', fontsize='small')\n",
    "\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "conflict_flights = testerrrrr['crosses_conflict'].sum()\n",
    "total_flights = len(testerrrrr)\n",
    "print(f\"Total flights: {total_flights}\")\n",
    "print(f\"Conflict-affected flights: {conflict_flights}\")\n",
    "print(f\"Percentage affected: {conflict_flights/total_flights*100:.1f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d8fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting testcode\n",
    "''''''\n",
    "# plotting the flight paths with conflict zones\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import LineString, Point\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "from geopy import distance\n",
    "from geographiclib.geodesic import Geodesic\n",
    "\n",
    "testerrrrr = pd.read_pickle('/scratch/omg28/Data/2023-01-01_to_2023-01-31_labeled.pkl')[0:10]\n",
    "\n",
    "def create_geodesic_line(lon1, lat1, lon2, lat2, num_points=100):\n",
    "    \"\"\"Create a geodesic line between two points using WGS84 ellipsoid\"\"\"\n",
    "    geod = Geodesic.WGS84\n",
    "    line = geod.InverseLine(lat1, lon1, lat2, lon2)\n",
    "    \n",
    "    # Generate points along the geodesic\n",
    "    points = []\n",
    "    for i in range(num_points):\n",
    "        s = (i / (num_points - 1)) * line.s13\n",
    "        g = line.Position(s)\n",
    "        points.append((g['lon2'], g['lat2']))\n",
    "    \n",
    "    return points\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Load world boundaries\n",
    "try:\n",
    "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "except (AttributeError, KeyError):\n",
    "    world = gpd.read_file('ne_110m_admin_0_countries.zip')\n",
    "\n",
    "# Plot world map as background (only east of prime meridian)\n",
    "world_east = world[world.geometry.bounds.maxx > 0]\n",
    "world_east.plot(ax=ax, color='lightgray', edgecolor='white', alpha=0.7)\n",
    "\n",
    "# Create conflict zone masks (only east of prime meridian)\n",
    "name_col = 'NAME' if 'NAME' in world.columns else 'name'\n",
    "conflict_zones_east = conflict_zones[conflict_zones.geometry.bounds.maxx > 0]\n",
    "conflict_buffered_east = conflict_zones_east.geometry.buffer(1.0)\n",
    "conflict_buffered_east.plot(ax=ax, color='red', alpha=0.3, label='Conflict Zones (1° Buffer)')\n",
    "\n",
    "# Plot flight paths with geodesic routes (only if both endpoints east of prime meridian)\n",
    "for idx, flight in testerrrrr.iterrows():\n",
    "    if flight['estdeparturelong'] > 0 and flight['estarrivallong'] > 0:\n",
    "        geodesic_points = create_geodesic_line(\n",
    "            flight['estdeparturelong'], flight['estdeparturelat'],\n",
    "            flight['estarrivallong'], flight['estarrivallat']\n",
    "        )\n",
    "        points = np.array(geodesic_points)\n",
    "        lons = points[:, 0]\n",
    "        lats = points[:, 1]\n",
    "\n",
    "        # Find where the longitude jumps more than 180 degrees (dateline crossing)\n",
    "        lon_diff = np.abs(np.diff(lons))\n",
    "        split_indices = np.where(lon_diff > 180)[0] + 1\n",
    "\n",
    "        # Split the points at the dateline\n",
    "        segments = np.split(points, split_indices)\n",
    "\n",
    "        color = 'orange' if flight['crosses_conflict'] else 'blue'\n",
    "        alpha = 0.8 if flight['crosses_conflict'] else 0.3\n",
    "\n",
    "        for seg in segments:\n",
    "            if len(seg) < 2:\n",
    "                continue\n",
    "            line = LineString(seg)\n",
    "            gdf_line = gpd.GeoDataFrame([1], geometry=[line])\n",
    "            gdf_line.plot(ax=ax, color=color, alpha=alpha, linewidth=0.5)\n",
    "\n",
    "# Add labels and formatting\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('Geodesic Flight Paths with Conflict Zone Analysis (East of Prime Meridian)')\n",
    "ax.set_xlim(left=0)  # Only show east of prime meridian\n",
    "\n",
    "# Create custom legend\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='blue', alpha=0.3, label='Normal Flights'),\n",
    "    Line2D([0], [0], color='orange', alpha=0.8, label='Conflict-Affected Flights'),\n",
    "    plt.Rectangle((0, 0), 1, 1, facecolor='red', alpha=0.3, label='Conflict Zones (1° Buffer)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower left', fontsize='small')\n",
    "\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a967f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the proportion of flights affected by conflict across all years\n",
    "total_conflict_flights = 0\n",
    "total_all_flights = 0\n",
    "\n",
    "# Loop through all months in the analysis period\n",
    "for start_time_str_loop in pd.date_range(start=pd.to_datetime(start_time_str), end=pd.to_datetime(stop_time_str), freq='MS', tz='UTC'):\n",
    "    stop_time_str_loop = (start_time_str_loop + pd.offsets.MonthEnd(1)).replace(hour=23, minute=59, second=59)\n",
    "    start_time_simple_loop = pd.to_datetime(start_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    stop_time_simple_loop = pd.to_datetime(stop_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Load the labeled flight data for this month\n",
    "    try:\n",
    "        month_flights = pd.read_pickle(f'/scratch/omg28/Data/{start_time_simple_loop}_to_{stop_time_simple_loop}_labeled.pkl')\n",
    "        \n",
    "        # Count flights for this month\n",
    "        month_conflict_flights = month_flights['crosses_conflict'].sum()\n",
    "        month_total_flights = len(month_flights)\n",
    "        \n",
    "        total_conflict_flights += month_conflict_flights\n",
    "        total_all_flights += month_total_flights\n",
    "        \n",
    "        print(f\"{start_time_str_loop.strftime('%B %Y')}: {month_conflict_flights:,} conflict-affected flights out of {month_total_flights:,} total flights ({month_conflict_flights/month_total_flights*100:.2f}%)\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: No labeled data found for {start_time_str_loop.strftime('%B %Y')}\")\n",
    "        continue\n",
    "\n",
    "# Calculate overall statistics\n",
    "if total_all_flights > 0:\n",
    "    overall_percentage = total_conflict_flights / total_all_flights * 100\n",
    "    print(f\"\\nOverall {analysis_year} Summary:\")\n",
    "    print(f\"Total flights: {total_all_flights:,}\")\n",
    "    print(f\"Conflict-affected flights: {total_conflict_flights:,}\")\n",
    "    print(f\"Overall percentage affected: {overall_percentage:.2f}%\")\n",
    "else:\n",
    "    print(\"No flight data found for the analysis period\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e303638c",
   "metadata": {},
   "source": [
    "# Section 5: parse the engine data and emissions data and add it to the aircraft performance model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a921d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data from the saved engine data .csv files for analysis. Saves this dataframe to a .pkl file\n",
    "# Setting \"remove_superseded\" to True will remove all engines that are superseded by an updated emissions model\n",
    "engine_models = get_engine_data(update_superseded=False)\n",
    "engine_models_removed = get_engine_data(update_superseded=True)\n",
    "# calculate how many engines were removed\n",
    "print(\"Number of engines that would be removed: \" + str(len(engine_models['Engine Identification'].unique()) - len(engine_models_removed['Engine Identification'].unique())))\n",
    "# print the list of the engine models that are in engine_models_kept but not in engine_models_removed\n",
    "superseded_not_replaced = engine_models[~engine_models['Engine Identification'].isin(engine_models_removed['Engine Identification'])].get('Engine Identification').unique()\n",
    "print(\"List of engines that were removed: \" + str(superseded_not_replaced))\n",
    "# POTENTIAL FIX: REMOVE ENGINES THAT HAVE BEEN SUPERSEDED BY AN UPDATED EMISSIONS MODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60bd30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the aircraft performance model data from the saved .csv file\n",
    "performance_models = pd.read_csv(\"aircraft_performance_table_filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff54bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts the powerplant data from the performance model dataframe and appends it to new columns in the performance model dataframe\n",
    "allowed_manufacturers = engine_models['Manufacturer'].unique().tolist()\n",
    "performance_models = perf_model_powerplant_parser(performance_models, coerce_manufacturer=True, allowed_manufacturers=allowed_manufacturers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f83aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12s - attempts to match the powerplants extracted from the EUCONTROL aircraft performance model with an engine model in the engine emissions database\n",
    "# reset the index of engine_models so that 'UID No' becomes a column rather than the index, otherwise the merge will not work\n",
    "engine_models.reset_index(inplace=True)\n",
    "# attempts to match the powerplants extracted from the EUCONTROL aircraft performance model with an engine model in the engine emissions database\n",
    "performance_models_matched = match_engine_to_emissions_db(performance_models, engine_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from all rows without a matched engine model\n",
    "performance_models_matched = performance_models_matched.dropna(subset=['matched_engine_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaaa309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of performance models whose engines match superseded engines\n",
    "print(\"Number of performance models with engines that are superseded: \" + str(len(performance_models_matched[performance_models_matched['matched_engine_id'].isin(superseded_not_replaced)])))\n",
    "print(\"Number of performance models with engines that are NOT superseded: \" + str(len(performance_models_matched[~performance_models_matched['matched_engine_id'].isin(superseded_not_replaced)])))\n",
    "print(\"aircraft performance models with engines that are superseded: \\n\" + str(performance_models_matched[performance_models_matched['matched_engine_id'].isin(superseded_not_replaced)].get('typecode')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28deb160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the aircraft performance model data with the engine emissions database data\n",
    "performance_and_emissions_model = pd.merge(performance_models_matched, engine_models, how='inner', left_on='matched_engine_uid_no', right_on='UID No')\n",
    "pd.to_pickle(performance_and_emissions_model, \"performance_and_emissions_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c61dadc",
   "metadata": {},
   "source": [
    "# Section 6: generate and visualize flightpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f697208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance_data=pd.read_pickle('aircraft_performance_data_table.pkl')\n",
    "performance_and_emissions_model = pd.read_pickle('performance_and_emissions_model.pkl')\n",
    "aircraft_typecode = 'A320'\n",
    "flightpath = generate_flightpath(aircraft_typecode, 500, performance_and_emissions_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Sum durations from the flightpath dictionary for each phase\n",
    "climb_time = (flightpath['climb'].get('t_climb_0_5', 0) +\n",
    "              flightpath['climb'].get('t_climb_5_10', 0) +\n",
    "              flightpath['climb'].get('t_climb_10_15', 0) +\n",
    "              flightpath['climb'].get('t_climb_15_24', 0) +\n",
    "              flightpath['climb'].get('t_climb_ceil', 0))\n",
    "\n",
    "descent_time = (flightpath['descent'].get('t_descent_5_0', 0) +\n",
    "                flightpath['descent'].get('t_descent_10_5', 0) +\n",
    "                flightpath['descent'].get('t_descent_15_10', 0) +\n",
    "                flightpath['descent'].get('t_descent_24_15', 0) +\n",
    "                flightpath['descent'].get('t_descent_ceil', 0))\n",
    "\n",
    "cruise_time = flightpath['cruise'].get('t_cruise', 0)\n",
    "\n",
    "# Prepare data for the barchart\n",
    "phases = ['Climb', 'Cruise', 'Descent']\n",
    "durations = [climb_time, cruise_time, descent_time]\n",
    "\n",
    "# Optionally convert seconds to minutes (or leave in seconds)\n",
    "# durations = [t / 60.0 for t in durations]\n",
    "\n",
    "plt.figure()\n",
    "bars = plt.bar(phases, durations, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.xlabel('Flight Phases')\n",
    "plt.ylabel('Duration (seconds)')\n",
    "plt.title(f'Duration of Each Flight Leg ({aircraft_typecode})')\n",
    "\n",
    "# Annotate bars with duration values\n",
    "for bar, duration in zip(bars, durations):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height,\n",
    "             f'{duration:.0f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016bae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break down the climb phases\n",
    "climb_labels = ['Climb 0-5', 'Climb 5-10', 'Climb 10-15', 'Climb 15-24', 'Climb Ceil']\n",
    "climb_times = [\n",
    "    flightpath['climb'].get('t_climb_0_5', 0),\n",
    "    flightpath['climb'].get('t_climb_5_10', 0),\n",
    "    flightpath['climb'].get('t_climb_10_15', 0),\n",
    "    flightpath['climb'].get('t_climb_15_24', 0),\n",
    "    flightpath['climb'].get('t_climb_ceil', 0)\n",
    "]\n",
    "\n",
    "# Break down the descent phases\n",
    "descent_labels = ['Descent 5-0', 'Descent 10-5', 'Descent 15-10', 'Descent 24-15', 'Descent Ceil']\n",
    "descent_times = [\n",
    "    flightpath['descent'].get('t_descent_5_0', 0),\n",
    "    flightpath['descent'].get('t_descent_10_5', 0),\n",
    "    flightpath['descent'].get('t_descent_15_10', 0),\n",
    "    flightpath['descent'].get('t_descent_24_15', 0),\n",
    "    flightpath['descent'].get('t_descent_ceil', 0)\n",
    "]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Climb segments chart\n",
    "plt.subplot(1, 2, 1)\n",
    "bars_climb = plt.bar(climb_labels, climb_times, color='skyblue')\n",
    "plt.xlabel('Climb Segments')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title(f'Climb Phase Breakdown ({aircraft_typecode})')\n",
    "for bar, t in zip(bars_climb, climb_times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{t:.1f}', ha='center', va='bottom')\n",
    "\n",
    "# Descent segments chart\n",
    "plt.subplot(1, 2, 2)\n",
    "bars_descent = plt.bar(descent_labels, descent_times, color='salmon')\n",
    "plt.xlabel('Descent Segments')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title(f'Descent Phase Breakdown ({aircraft_typecode})')\n",
    "for bar, t in zip(bars_descent, descent_times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{t:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ef7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Overall phase distances (in meters)\n",
    "climb_distance = (np.atleast_1d(flightpath['climb'].get('s_climb_0_5', 0))[0] +\n",
    "                  np.atleast_1d(flightpath['climb'].get('s_climb_5_10', 0))[0] +\n",
    "                  np.atleast_1d(flightpath['climb'].get('s_climb_10_15', 0))[0] +\n",
    "                  np.atleast_1d(flightpath['climb'].get('s_climb_15_24', 0))[0] +\n",
    "                  np.atleast_1d(flightpath['climb'].get('s_climb_ceil', 0))[0])\n",
    "\n",
    "cruise_distance = np.atleast_1d(flightpath['cruise'].get('s_cruise', 0))[0]\n",
    "\n",
    "descent_distance = (np.atleast_1d(flightpath['descent'].get('s_descent_5_0', 0))[0] +\n",
    "                    np.atleast_1d(flightpath['descent'].get('s_descent_10_5', 0))[0] +\n",
    "                    np.atleast_1d(flightpath['descent'].get('s_descent_15_10', 0))[0] +\n",
    "                    np.atleast_1d(flightpath['descent'].get('s_descent_24_15', 0))[0] +\n",
    "                    np.atleast_1d(flightpath['descent'].get('s_descent_ceil', 0))[0])\n",
    "\n",
    "phases = ['Climb', 'Cruise', 'Descent']\n",
    "distances = [climb_distance, cruise_distance, descent_distance]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "overall_bars = plt.bar(phases, distances, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.xlabel('Flight Phases')\n",
    "plt.ylabel('Distance (meters)')\n",
    "plt.title(f'Distance Covered in Each Flight Phase ({aircraft_typecode})')\n",
    "\n",
    "# Annotate overall bars with distance values\n",
    "for bar, dist in zip(overall_bars, distances):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{dist:,.0f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Breakdown of distances for each segment\n",
    "\n",
    "# For Climb Phase\n",
    "climb_keys = ['s_climb_0_5', 's_climb_5_10', 's_climb_10_15', 's_climb_15_24', 's_climb_ceil']\n",
    "climb_distances = []\n",
    "for key in climb_keys:\n",
    "    value = flightpath['climb'].get(key, np.array([0]))\n",
    "    # value is expected to be a numpy array; extract its first element if so\n",
    "    if isinstance(value, np.ndarray):\n",
    "        climb_distances.append(value[0])\n",
    "    else:\n",
    "        climb_distances.append(value)\n",
    "\n",
    "# For Descent Phase\n",
    "descent_keys = ['s_descent_5_0', 's_descent_10_5', 's_descent_15_10', 's_descent_24_15', 's_descent_ceil']\n",
    "descent_distances = []\n",
    "for key in descent_keys:\n",
    "    value = flightpath['descent'].get(key, np.array([0]))\n",
    "    if isinstance(value, np.ndarray):\n",
    "        descent_distances.append(value[0])\n",
    "    else:\n",
    "        descent_distances.append(value)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Climb segments chart\n",
    "plt.subplot(1, 2, 1)\n",
    "bars_climb = plt.bar(climb_labels, climb_distances, color='skyblue')\n",
    "plt.xlabel('Climb Segments')\n",
    "plt.ylabel('Distance (meters)')\n",
    "plt.title(f'Climb Phase Distance Breakdown ({aircraft_typecode})')\n",
    "for bar, d in zip(bars_climb, climb_distances):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{d:,.0f}', ha='center', va='bottom')\n",
    "\n",
    "# Descent segments chart\n",
    "plt.subplot(1, 2, 2)\n",
    "bars_descent = plt.bar(descent_labels, descent_distances, color='salmon')\n",
    "plt.xlabel('Descent Segments')\n",
    "plt.ylabel('Distance (meters)')\n",
    "plt.title(f'Descent Phase Distance Breakdown ({aircraft_typecode})')\n",
    "for bar, d in zip(bars_descent, descent_distances):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{d:,.0f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49eca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build cumulative distance and altitude lists.\n",
    "# Starting at runway: (distance, altitude) = (0, 0)\n",
    "distances_list = [0]\n",
    "altitudes_list = [0]\n",
    "\n",
    "# Climb phase:\n",
    "# First climb segment (0-5):\n",
    "cum_distance = distances_list[-1]\n",
    "for key in flightpath:\n",
    "    for key2 in flightpath[key]:\n",
    "        if key2.startswith('s_'):\n",
    "            cum_distance += flightpath[key][key2]\n",
    "            distances_list.append(cum_distance)\n",
    "            altitudes_list.append(flightpath[key][f'{key2.replace('s_', 'h_')}_end'])\n",
    "\n",
    "# Plot the altitude profile vs. cumulative flight distance.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(distances_list, altitudes_list, marker='o', linestyle='-')\n",
    "plt.xlabel('Cumulative Flight Distance (meters)')\n",
    "plt.ylabel('Flight Altitude (meters)')\n",
    "plt.title(f'Flight Altitude vs. Flight Distance ({aircraft_typecode})')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 25 flight distances (km) with more samples at shorter distances using a non-linear (quadratic) spacing.\n",
    "max_distances = pd.read_csv(f'/scratch/omg28/Data/no_track2024/{start_time_simple}_to_{stop_time_simple}_typecode_max_distances.csv')\n",
    "max_distance_km = max_distances.loc[max_distances['typecode'] == aircraft_typecode, 'max_distance_km'].max()\n",
    "num_flightpaths = 5\n",
    "\n",
    "# Use quadratic spacing: more points at the start, fewer at the end\n",
    "# Use logarithmic spacing: more points at the start, fewer at the end\n",
    "flight_distances = np.logspace(np.log10(200), np.log10(1000), num_flightpaths)\n",
    "flightpaths = [0] * num_flightpaths\n",
    "\n",
    "\n",
    "for ii in range(num_flightpaths):\n",
    "    d_km = flight_distances[ii]\n",
    "    fp = generate_flightpath(aircraft_typecode, d_km, performance_and_emissions_model)\n",
    "    flightpaths[ii] = fp\n",
    "\n",
    "plot_flightpaths(flightpaths, aircraft_typecode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1168ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate a grid of flight profiles varying both cruising altitude and total flight distance\n",
    "max_cruise_altitude_ft = performance_and_emissions_model.loc[\n",
    "    performance_and_emissions_model['typecode'] == aircraft_typecode, 'cruise_Ceiling'\n",
    "].max() * 1e2\n",
    "\n",
    "num_altitudes = 10\n",
    "num_distances = 10\n",
    "cruise_altitudes_ft = np.linspace(18000, max_cruise_altitude_ft, num_altitudes)\n",
    "# Use quadratic spacing for grid as well\n",
    "flight_distances_km = np.logspace(np.log10(200), np.log10(max_distance_km), num_distances)\n",
    "\n",
    "flightpaths_grid = []\n",
    "labels = []\n",
    "\n",
    "for alt_ft in cruise_altitudes_ft:\n",
    "    for d_km in flight_distances_km:\n",
    "        fp = generate_flightpath(aircraft_typecode, d_km, performance_and_emissions_model, cruise_altitude_ft=alt_ft)\n",
    "        flightpaths_grid.append(fp)\n",
    "        labels.append(f\"{int(alt_ft/1000)}kft, {int(d_km)}km\")\n",
    "\n",
    "# Example: plot total NOx emissions for each profile\n",
    "total_nox = []\n",
    "for fp in flightpaths_grid:\n",
    "    nox = 0\n",
    "    for k in ['NOx_climb_0_5', 'NOx_climb_5_10', 'NOx_climb_10_15', 'NOx_climb_15_24', 'NOx_climb_ceil']:\n",
    "        nox += fp['climb'].get(k, 0)\n",
    "    for k in ['NOx_descent_ceil', 'NOx_descent_24_15', 'NOx_descent_15_10', 'NOx_descent_10_5', 'NOx_descent_5_0']:\n",
    "        nox += fp['descent'].get(k, 0)\n",
    "    nox += fp.get('cruise', {}).get('NOx_cruise', 0)\n",
    "    total_nox.append(nox)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(labels, total_nox, color='purple')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Total NOx Emissions (g)')\n",
    "plt.title(f'Total NOx Emissions for Various Altitudes and Distances ({aircraft_typecode})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024efcdd",
   "metadata": {},
   "source": [
    "### Visualize NOx Emissions by Flight Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04149b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize NOx emissions for each climb and descent phase\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "climb_nox_labels = ['Climb 0-5', 'Climb 5-10', 'Climb 10-15', 'Climb 15-24', 'Climb Ceil']\n",
    "climb_nox_keys = ['NOx_climb_0_5', 'NOx_climb_5_10', 'NOx_climb_10_15', 'NOx_climb_15_24', 'NOx_climb_ceil']\n",
    "climb_nox_values = [flightpath['climb'].get(k, 0) for k in climb_nox_keys]\n",
    "\n",
    "descent_nox_labels = ['Descent Ceil', 'Descent 24-15', 'Descent 15-10', 'Descent 10-5', 'Descent 5-0']\n",
    "descent_nox_keys = ['NOx_descent_ceil', 'NOx_descent_24_15', 'NOx_descent_15_10', 'NOx_descent_10_5', 'NOx_descent_5_0']\n",
    "descent_nox_values = [flightpath['descent'].get(k, 0) for k in descent_nox_keys]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "bars_climb = plt.bar(climb_nox_labels, climb_nox_values, color='deepskyblue')\n",
    "plt.xlabel('Climb Segments')\n",
    "plt.ylabel('NOx Emissions (g)')\n",
    "plt.title(f'NOx Emissions During Climb ({aircraft_typecode})')\n",
    "for bar, val in zip(bars_climb, climb_nox_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{val:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "bars_descent = plt.bar(descent_nox_labels, descent_nox_values, color='orange')\n",
    "plt.xlabel('Descent Segments')\n",
    "plt.ylabel('NOx Emissions (g)')\n",
    "plt.title(f'NOx Emissions During Descent ({aircraft_typecode})')\n",
    "for bar, val in zip(bars_descent, descent_nox_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{val:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cumulative NOx Emissions vs. Flight Distance\n",
    "num_flightpaths = 25\n",
    "flight_distances = np.linspace(200, 5000, num_flightpaths)\n",
    "cumulative_nox = []\n",
    "for d_km in flight_distances:\n",
    "    fp = generate_flightpath(aircraft_typecode, d_km, performance_and_emissions_model)\n",
    "    total_nox = 0\n",
    "    for k in ['NOx_climb_0_5', 'NOx_climb_5_10', 'NOx_climb_10_15', 'NOx_climb_15_24', 'NOx_climb_ceil']:\n",
    "        total_nox += fp['climb'].get(k, 0)\n",
    "    for k in ['NOx_descent_ceil', 'NOx_descent_24_15', 'NOx_descent_15_10', 'NOx_descent_10_5', 'NOx_descent_5_0']:\n",
    "        total_nox += fp['descent'].get(k, 0)\n",
    "    total_nox += fp.get('cruise', {}).get('NOx_cruise', 0)\n",
    "    cumulative_nox.append(total_nox)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(flight_distances, cumulative_nox, marker='o', color='purple')\n",
    "plt.xlabel('Flight Distance (km)')\n",
    "plt.ylabel('Cumulative NOx Emissions (g)')\n",
    "plt.title(f'Cumulative NOx Emissions vs. Flight Distance ({aircraft_typecode})')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893eafcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay altitude profile on the same x-axis as NOx flux, using a secondary y-axis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Build lists for distance and NOx flux (g/s) at each segment\n",
    "segment_distances = [0]\n",
    "segment_nox_flux = []\n",
    "\n",
    "# Climb segments\n",
    "climb_keys = [\n",
    "    ('s_climb_0_5', 't_climb_0_5', 'NOx_climb_0_5'),\n",
    "    ('s_climb_5_10', 't_climb_5_10', 'NOx_climb_5_10'),\n",
    "    ('s_climb_10_15', 't_climb_10_15', 'NOx_climb_10_15'),\n",
    "    ('s_climb_15_24', 't_climb_15_24', 'NOx_climb_15_24'),\n",
    "    ('s_climb_ceil', 't_climb_ceil', 'NOx_climb_ceil'),\n",
    "]\n",
    "cum_dist = 0\n",
    "for s_key, t_key, nox_key in climb_keys:\n",
    "    s = flightpath['climb'].get(s_key, 0)\n",
    "    t = flightpath['climb'].get(t_key, 0)\n",
    "    nox = flightpath['climb'].get(nox_key, 0)\n",
    "    if t > 0 and s > 0:\n",
    "        cum_dist += s\n",
    "        segment_distances.append(cum_dist)\n",
    "        segment_nox_flux.append(nox / t)\n",
    "\n",
    "# Cruise segment (if available)\n",
    "s_cruise = flightpath['cruise'].get('s_cruise', 0)\n",
    "t_cruise = flightpath['cruise'].get('t_cruise', 0)\n",
    "nox_cruise = flightpath['cruise'].get('NOx_cruise', 0)\n",
    "if t_cruise > 0 and s_cruise > 0:\n",
    "    cum_dist += s_cruise\n",
    "    segment_distances.append(cum_dist)\n",
    "    segment_nox_flux.append(nox_cruise / t_cruise if t_cruise > 0 else 0)\n",
    "\n",
    "# Descent segments\n",
    "descent_keys = [\n",
    "    ('s_descent_ceil', 't_descent_ceil', 'NOx_descent_ceil'),\n",
    "    ('s_descent_24_15', 't_descent_24_15', 'NOx_descent_24_15'),\n",
    "    ('s_descent_15_10', 't_descent_15_10', 'NOx_descent_15_10'),\n",
    "    ('s_descent_10_5', 't_descent_10_5', 'NOx_descent_10_5'),\n",
    "    ('s_descent_5_0', 't_descent_5_0', 'NOx_descent_5_0'),\n",
    "]\n",
    "for s_key, t_key, nox_key in descent_keys:\n",
    "    s = flightpath['descent'].get(s_key, 0)\n",
    "    t = flightpath['descent'].get(t_key, 0)\n",
    "    nox = flightpath['descent'].get(nox_key, 0)\n",
    "    if t > 0 and s > 0:\n",
    "        cum_dist += s\n",
    "        segment_distances.append(cum_dist)\n",
    "        segment_nox_flux.append(nox / t)\n",
    "\n",
    "# Ensure x and y have the same length by removing the initial 0 from segment_distances.\n",
    "segment_distances = segment_distances[1:]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.step(segment_distances, segment_nox_flux, where='post', marker='o')\n",
    "plt.xlabel('Cumulative Flight Distance (meters)')\n",
    "plt.ylabel('Instantaneous NOx Flux (g/s)')\n",
    "plt.title(f'Instantaneous NOx Flux vs. Flight Distance ({aircraft_typecode})')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c044cc",
   "metadata": {},
   "source": [
    "## Model Comparison with 5-Fold Cross-Validation, Hyperparameter Tuning, and Model Saving\n",
    "This section compares XGBoost, polynomial regression, and nonlinear curve fitting for NOx flux prediction, using 5-fold cross-validation and hyperparameter tuning. Only typecodes present in the `performance_and_emissions_model` database are included. Trained models and their best parameters are saved to disk for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433e6ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "max_distances = pd.read_csv(f'{output_dir}/aircraftdb/{start_time_simple}_to_{stop_time_simple}_typecode_max_distances.csv')\n",
    "performance_and_emissions_model = pd.read_pickle('performance_and_emissions_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "num_altitudes = 6; num_distances = 12\n",
    "typecodes = performance_and_emissions_model['typecode'].unique()\n",
    "model_save_dir = 'saved_models_nox_flux'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "def get_X_y(typecode):\n",
    "    max_dist_row = max_distances[max_distances['typecode'] == typecode]\n",
    "    if max_dist_row.empty: return None, None\n",
    "    max_distance_km = max_dist_row['max_distance_km'].values[0]\n",
    "    max_cruise_altitude_ft = performance_and_emissions_model.loc[performance_and_emissions_model['typecode'] == typecode, 'cruise_Ceiling'].max() * 1e2\n",
    "    if pd.isnull(max_cruise_altitude_ft) or pd.isnull(max_distance_km): return None, None\n",
    "\n",
    "    cruise_altitudes_ft = np.linspace(18000, max_cruise_altitude_ft, num_altitudes)\n",
    "    flight_distances_km = np.logspace(np.log10(200), np.log10(max_distance_km), num_distances)\n",
    "    X, y = [], []\n",
    "    for alt_ft in cruise_altitudes_ft:\n",
    "        for d_km in flight_distances_km:\n",
    "            try:\n",
    "                fp = generate_flightpath(typecode, d_km, performance_and_emissions_model, cruise_altitude_ft=alt_ft)\n",
    "                total_nox = 0; total_time = 0\n",
    "                for nox_key, t_key in zip(['NOx_climb_0_5','NOx_climb_5_10','NOx_climb_10_15','NOx_climb_15_24','NOx_climb_ceil'],['t_climb_0_5','t_climb_5_10','t_climb_10_15','t_climb_15_24','t_climb_ceil']):\n",
    "                    total_nox += fp.get('climb', {}).get(nox_key, 0)\n",
    "                    total_time += fp.get('climb', {}).get(t_key, 0)\n",
    "                for nox_key, t_key in zip(['NOx_descent_ceil','NOx_descent_24_15','NOx_descent_15_10','NOx_descent_10_5','NOx_descent_5_0'],['t_descent_ceil','t_descent_24_15','t_descent_15_10','t_descent_10_5','t_descent_5_0']):\n",
    "                    total_nox += fp.get('descent', {}).get(nox_key, 0)\n",
    "                    total_time += fp.get('descent', {}).get(t_key, 0)\n",
    "                total_nox += fp.get('cruise', {}).get('NOx_cruise', 0)\n",
    "                total_time += fp.get('cruise', {}).get('t_cruise', 0)\n",
    "                if total_time > 0:\n",
    "                    mean_nox_flux = total_nox / total_time\n",
    "                    X.append([d_km, alt_ft])\n",
    "                    y.append(mean_nox_flux)\n",
    "            except Exception: continue\n",
    "    X = np.array(X); y = np.array(y)\n",
    "    if len(y) < 8: return None, None\n",
    "    return X, y\n",
    "\n",
    "# XGBoost with 5-fold CV and Grid Search\n",
    "xgb_results = {}\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.05, 0.1, 0.2]\n",
    "}\n",
    "for typecode in typecodes:\n",
    "    X, y = get_X_y(typecode)\n",
    "    if X is None: continue\n",
    "    model = XGBRegressor(random_state=42)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    grid = GridSearchCV(model, xgb_param_grid, cv=kf, scoring='r2', n_jobs=-1)\n",
    "    grid.fit(X, y)\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = cross_val_predict(best_model, X, y, cv=kf)\n",
    "    r2s = cross_val_score(best_model, X, y, cv=kf, scoring='r2')\n",
    "    rmses = []\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        best_model.fit(X[train_idx], y[train_idx])\n",
    "        y_test_pred = best_model.predict(X[test_idx])\n",
    "        rmses.append(np.sqrt(mean_squared_error(y[test_idx], y_test_pred)))\n",
    "    xgb_results[typecode] = {\n",
    "        'r2_mean': np.mean(r2s), 'r2_std': np.std(r2s),\n",
    "        'rmse_mean': np.mean(rmses), 'rmse_std': np.std(rmses),\n",
    "        'n_samples': len(y), 'best_params': grid.best_params_\n",
    "    }\n",
    "    # Save model\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    fout = os.path.join(model_save_dir, f'xgb_{typecode}.ubj')\n",
    "    best_model.save_model(str(fout))\n",
    "    print(f\"[XGBoost CV] {typecode}: R2={np.mean(r2s):.3f}±{np.std(r2s):.3f}, RMSE={np.mean(rmses):.3f}±{np.std(rmses):.3f}, N={len(y)}, Best={grid.best_params_}\")\n",
    "\n",
    "# Polynomial Regression (degree 2) with 5-fold CV and Ridge/Lasso/ElasticNet Grid Search\n",
    "poly_results = {}\n",
    "from sklearn.pipeline import Pipeline\n",
    "poly_param_grid = {\n",
    "    'regressor': [Ridge(), Lasso(), ElasticNet()],\n",
    "    'regressor__alpha': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "for typecode in typecodes:\n",
    "    X, y = get_X_y(typecode)\n",
    "    if X is None: continue\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    X_poly = poly.fit_transform(X_scaled)\n",
    "    pipe = Pipeline([\n",
    "        ('regressor', Ridge())\n",
    "    ])\n",
    "    grid = GridSearchCV(pipe, poly_param_grid, cv=5, scoring='r2', n_jobs=-1, error_score='raise')\n",
    "    grid.fit(X_poly, y)\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = cross_val_predict(best_model, X_poly, y, cv=5)\n",
    "    r2s = cross_val_score(best_model, X_poly, y, cv=5, scoring='r2')\n",
    "    rmses = []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_idx, test_idx in kf.split(X_poly):\n",
    "        best_model.fit(X_poly[train_idx], y[train_idx])\n",
    "        y_test_pred = best_model.predict(X_poly[test_idx])\n",
    "        rmses.append(np.sqrt(mean_squared_error(y[test_idx], y_test_pred)))\n",
    "    poly_results[typecode] = {\n",
    "        'r2_mean': np.mean(r2s), 'r2_std': np.std(r2s),\n",
    "        'rmse_mean': np.mean(rmses), 'rmse_std': np.std(rmses),\n",
    "        'n_samples': len(y), 'best_params': grid.best_params_\n",
    "    }\n",
    "    # Save model and scaler/polynomial\n",
    "    with open(os.path.join(model_save_dir, f'poly2_{typecode}.pkl'), 'wb') as f:\n",
    "        pickle.dump({'model': best_model, 'scaler': scaler, 'poly': poly}, f)\n",
    "    print(f\"[Poly2 CV] {typecode}: R2={np.mean(r2s):.3f}±{np.std(r2s):.3f}, RMSE={np.mean(rmses):.3f}±{np.std(rmses):.3f}, N={len(y)}, Best={grid.best_params_}\")\n",
    "\n",
    "# Nonlinear Curve Fitting (Power Law) with 5-fold CV and Model Saving\n",
    "def powerlaw_func(X, a, b, c):\n",
    "    d, h = X\n",
    "    return a * np.power(d, b) * np.power(h, c)\n",
    "nl_results = {}\n",
    "for typecode in typecodes:\n",
    "    X, y = get_X_y(typecode)\n",
    "    if X is None: continue\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    r2s = []; rmses = []; params_list = []\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        try:\n",
    "            popt, _ = curve_fit(lambda X1, a, b, c: powerlaw_func((X1[:,0], X1[:,1]), a, b, c), X[train_idx], y[train_idx], p0=[1,1,1], maxfev=10000)\n",
    "            y_test_pred = powerlaw_func((X[test_idx,0], X[test_idx,1]), *popt)\n",
    "            r2s.append(r2_score(y[test_idx], y_test_pred))\n",
    "            rmses.append(np.sqrt(mean_squared_error(y[test_idx], y_test_pred)))\n",
    "            params_list.append(popt)\n",
    "        except Exception as e:\n",
    "            r2s.append(np.nan); rmses.append(np.nan); params_list.append([np.nan, np.nan, np.nan])\n",
    "    # Save mean params for this typecode\n",
    "    mean_params = np.nanmean(np.array(params_list), axis=0)\n",
    "    nl_results[typecode] = {'r2_mean': np.nanmean(r2s), 'r2_std': np.nanstd(r2s), 'rmse_mean': np.nanmean(rmses), 'rmse_std': np.nanstd(rmses), 'n_samples': len(y), 'params': mean_params}\n",
    "    with open(os.path.join(model_save_dir, f'nonlinear_{typecode}.pkl'), 'wb') as f:\n",
    "        pickle.dump({'params': mean_params}, f)\n",
    "    print(f\"[Nonlinear CV] {typecode}: R2={np.nanmean(r2s):.3f}±{np.nanstd(r2s):.3f}, RMSE={np.nanmean(rmses):.3f}±{np.nanstd(rmses):.3f}, N={len(y)}, Params={mean_params}\")\n",
    "\n",
    "# Summary Table: Model Comparison for Each Typecode\n",
    "print(\"\\nTypecode | XGBoost R2±std | Poly2 R2±std | Nonlinear R2±std | XGBoost RMSE±std | Poly2 RMSE±std | Nonlinear RMSE±std\")\n",
    "for tc in typecodes:\n",
    "    xg = xgb_results.get(tc, {})\n",
    "    pl = poly_results.get(tc, {})\n",
    "    nl = nl_results.get(tc, {})\n",
    "    print(f\"{tc:8} | {xg.get('r2_mean',float('nan')):.3f}±{xg.get('r2_std',float('nan')):.3f} | {pl.get('r2_mean',float('nan')):.3f}±{pl.get('r2_std',float('nan')):.3f} | {nl.get('r2_mean',float('nan')):.3f}±{nl.get('r2_std',float('nan')):.3f} | {xg.get('rmse_mean',float('nan')):.3f}±{xg.get('rmse_std',float('nan')):.3f} | {pl.get('rmse_mean',float('nan')):.3f}±{pl.get('rmse_std',float('nan')):.3f} | {nl.get('rmse_mean',float('nan')):.3f}±{nl.get('rmse_std',float('nan')):.3f}\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0764e2",
   "metadata": {},
   "source": [
    "## Gridded NOx Emissions for January 2024\n",
    "This section generates a 3D grid (0.5° lat × 0.5° lon × 1000 ft altitude bins, up to 16,800 m/55,000 ft) of average NOx flux (kg/month) for all flights in January 2024, using the best (lowest RMSE) model for each aircraft type. Each flight's NOx is distributed along its trajectory, and emissions in each grid cell are averaged over the month, accounting for a 2-day atmospheric removal timescale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21570cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_and_emissions_model = pd.read_pickle('performance_and_emissions_model.pkl')\n",
    "\n",
    "for start_time_str_loop in pd.date_range(start=pd.to_datetime(start_time_str), end=pd.to_datetime(stop_time_str), freq='MS', tz='UTC'):\n",
    "    stop_time_str_loop = (start_time_str_loop + pd.offsets.MonthEnd(1)).replace(hour=23, minute=59, second=59)\n",
    "    process_month_emissions(\n",
    "        start_time_str_loop,\n",
    "        output_dir=output_dir,\n",
    "        performance_and_emissions_model=performance_and_emissions_model\n",
    "    )\n",
    "    print(f\"Generated emissions file for month: {start_time_str_loop.strftime('%Y-%m')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779cba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot monthly NOx emissions by altitude and latitude as a proportion of total emissions\n",
    "for start_time_str_loop in pd.date_range(start=pd.to_datetime(start_time_str), end=pd.to_datetime(stop_time_str), freq='MS', tz='UTC'):\n",
    "    stop_time_str_loop = (start_time_str_loop + pd.offsets.MonthEnd(1)).replace(hour=23, minute=59, second=59)\n",
    "    start_time_simple_loop = pd.to_datetime(start_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    stop_time_simple_loop = pd.to_datetime(stop_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Load the NOx emissions grid\n",
    "    nox_grid = np.load(f'/home/omg28/nethome/Data/emissions/{start_time_simple_loop}_to_{stop_time_simple_loop}_NOx_nowar.npy')\n",
    "    \n",
    "    # Sum over longitude to get a 2D array: (latitude, altitude)\n",
    "    nox_lat_alt = np.sum(nox_grid, axis=1)  # shape: (nlat, nalt)\n",
    "\n",
    "    # Compute the proportion of total emissions for each (lat, alt) bin\n",
    "    total_emissions = np.sum(nox_lat_alt)\n",
    "    if total_emissions > 0:\n",
    "        nox_lat_alt_prop = nox_lat_alt / total_emissions\n",
    "    else:\n",
    "        nox_lat_alt_prop = nox_lat_alt  # all zeros\n",
    "\n",
    "    # Get month and year for the plot title\n",
    "    month_year = pd.to_datetime(start_time_str_loop).strftime('%B %Y')\n",
    "\n",
    "    # Plot heatmap of proportions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(\n",
    "        nox_lat_alt_prop.T, \n",
    "        aspect='auto', \n",
    "        origin='lower',\n",
    "        extent=[lat_bins[0], lat_bins[-2], alt_bins_m[0], alt_bins_m[-2]],\n",
    "        cmap='turbo'\n",
    "    )\n",
    "    plt.colorbar(label='Proportion of Total NOx Emissions')\n",
    "    plt.xlabel('Latitude (degrees)')\n",
    "    plt.ylabel('Altitude (meters)')\n",
    "    plt.title(f'NOx Emissions Proportion Heatmap ({month_year}): Altitude vs Latitude')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a4e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each month in the analysis period\n",
    "for start_time_str_loop in pd.date_range(start=pd.to_datetime(start_time_str), end=pd.to_datetime(stop_time_str), freq='MS', tz='UTC'):\n",
    "    stop_time_str_loop = (start_time_str_loop + pd.offsets.MonthEnd(1)).replace(hour=23, minute=59, second=59)\n",
    "    start_time_simple_loop = pd.to_datetime(start_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    stop_time_simple_loop = pd.to_datetime(stop_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    month_year = pd.to_datetime(start_time_str_loop).strftime('%B %Y')\n",
    "\n",
    "    # Load the NOx emissions grid for this month\n",
    "    nox_grid = np.load(f'/home/omg28/nethome/Data/emissions/{start_time_simple_loop}_to_{stop_time_simple_loop}_NOx_nowar.npy')\n",
    "\n",
    "    # Sum over altitude to get total NOx flux per (lat, lon) cell\n",
    "    nox_lat_lon = np.sum(nox_grid, axis=2)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.imshow(\n",
    "        nox_lat_lon,\n",
    "        extent=[lon_bins[0], lon_bins[-2], lat_bins[0], lat_bins[-2]],\n",
    "        origin='lower',\n",
    "        aspect='auto',\n",
    "        cmap='gist_ncar'\n",
    "    )\n",
    "    plt.colorbar(label='NOx Emissions (kg/month)')\n",
    "    plt.xlabel('Longitude (degrees)')\n",
    "    plt.ylabel('Latitude (degrees)')\n",
    "    plt.title(f'Average Monthly NOx Flux (kg/month) vs Latitude and Longitude ({month_year})')\n",
    "    # Compute the proportion of total emissions for each (lat, lon) cell\n",
    "    total_emissions = np.sum(nox_lat_lon)\n",
    "    if total_emissions > 0:\n",
    "        nox_lat_lon_prop = nox_lat_lon / total_emissions\n",
    "    else:\n",
    "        nox_lat_lon_prop = nox_lat_lon  # all zeros\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Replace the image with the proportion heatmap\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    im = plt.imshow(\n",
    "        nox_lat_lon_prop,\n",
    "        extent=[lon_bins[0], lon_bins[-2], lat_bins[0], lat_bins[-2]],\n",
    "        origin='lower',\n",
    "        aspect='auto',\n",
    "        cmap='turbo'\n",
    "    )\n",
    "    plt.colorbar(im, label='Proportion of Total NOx Emissions')\n",
    "    plt.xlabel('Longitude (degrees)')\n",
    "    plt.ylabel('Latitude (degrees)')\n",
    "    plt.title(f'NOx Emissions Proportion Heatmap ({month_year}): Latitude vs Longitude')\n",
    "\n",
    "    # Draw a thin red outline around Ukraine\n",
    "    try:\n",
    "        world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    except (AttributeError, KeyError):\n",
    "        world = gpd.read_file('ne_110m_admin_0_countries.zip')\n",
    "    ukraine = world[world['NAME'] == 'Ukraine'] if 'NAME' in world.columns else world[world['name'] == 'Ukraine']\n",
    "    ax = plt.gca()\n",
    "    ukraine.boundary.plot(ax=ax, edgecolor='red', linewidth=1, zorder=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Draw a thin red outline around Ukraine\n",
    "\n",
    "    # Download or load a world country shapefile (naturalearth_lowres)\n",
    "    # If you have geopandas >= 1.0, download the shapefile manually and provide the path below\n",
    "    # Example: world = gpd.read_file('/path/to/ne_110m_admin_0_countries.shp')\n",
    "    # For convenience, try to use the 'naturalearth_lowres' from geopandas if available, else fallback to a downloaded file\n",
    "\n",
    "    try:\n",
    "        # Try the old method (may fail in geopandas >= 1.0)\n",
    "        world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    except (AttributeError, KeyError):\n",
    "        # Fallback: download and use the shapefile manually\n",
    "        # Please download from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/\n",
    "        # and set the correct path below\n",
    "        world = gpd.read_file('ne_110m_admin_0_countries.zip')\n",
    "\n",
    "    ukraine = world[world['NAME'] == 'Ukraine'] if 'NAME' in world.columns else world[world['name'] == 'Ukraine']\n",
    "\n",
    "    # Plot the outline on top of the map\n",
    "    ax = plt.gca()\n",
    "    ukraine.boundary.plot(ax=ax, edgecolor='red', linewidth=1, zorder=10)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each month in the analysis period\n",
    "for start_time_str_loop in pd.date_range(start=pd.to_datetime(start_time_str), end=pd.to_datetime(stop_time_str), freq='MS', tz='UTC'):\n",
    "    stop_time_str_loop = (start_time_str_loop + pd.offsets.MonthEnd(1)).replace(hour=23, minute=59, second=59)\n",
    "    start_time_simple_loop = pd.to_datetime(start_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    stop_time_simple_loop = pd.to_datetime(stop_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    month_year = pd.to_datetime(start_time_str_loop).strftime('%B %Y')\n",
    "\n",
    "    # Load the NOx emissions grid for this month\n",
    "    nox_grid = np.load(f'/home/omg28/nethome/Data/emissions/{start_time_simple_loop}_to_{stop_time_simple_loop}_NOx_war.npy')\n",
    "\n",
    "    # Sum over altitude to get total NOx flux per (lat, lon) cell\n",
    "    nox_lat_lon = np.sum(nox_grid, axis=2)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.imshow(\n",
    "        nox_lat_lon,\n",
    "        extent=[lon_bins[0], lon_bins[-2], lat_bins[0], lat_bins[-2]],\n",
    "        origin='lower',\n",
    "        aspect='auto',\n",
    "        cmap='gist_ncar'\n",
    "    )\n",
    "    plt.colorbar(label='NOx Emissions (kg/month)')\n",
    "    plt.xlabel('Longitude (degrees)')\n",
    "    plt.ylabel('Latitude (degrees)')\n",
    "    plt.title(f'Average Monthly NOx Flux (kg/month) vs Latitude and Longitude ({month_year})')\n",
    "    # Compute the proportion of total emissions for each (lat, lon) cell\n",
    "    total_emissions = np.sum(nox_lat_lon)\n",
    "    if total_emissions > 0:\n",
    "        nox_lat_lon_prop = nox_lat_lon / total_emissions\n",
    "    else:\n",
    "        nox_lat_lon_prop = nox_lat_lon  # all zeros\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Replace the image with the proportion heatmap\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    im = plt.imshow(\n",
    "        nox_lat_lon_prop,\n",
    "        extent=[lon_bins[0], lon_bins[-2], lat_bins[0], lat_bins[-2]],\n",
    "        origin='lower',\n",
    "        aspect='auto',\n",
    "        cmap='turbo'\n",
    "    )\n",
    "    plt.colorbar(im, label='Proportion of Total NOx Emissions')\n",
    "    plt.xlabel('Longitude (degrees)')\n",
    "    plt.ylabel('Latitude (degrees)')\n",
    "    plt.title(f'NOx Emissions Proportion Heatmap ({month_year}): Latitude vs Longitude')\n",
    "\n",
    "    # Draw a thin red outline around Ukraine\n",
    "    try:\n",
    "        world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    except (AttributeError, KeyError):\n",
    "        world = gpd.read_file('ne_110m_admin_0_countries.zip')\n",
    "    ukraine = world[world['NAME'] == 'Ukraine'] if 'NAME' in world.columns else world[world['name'] == 'Ukraine']\n",
    "    ax = plt.gca()\n",
    "    ukraine.boundary.plot(ax=ax, edgecolor='red', linewidth=1, zorder=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Draw a thin red outline around Ukraine\n",
    "\n",
    "    # Download or load a world country shapefile (naturalearth_lowres)\n",
    "    # If you have geopandas >= 1.0, download the shapefile manually and provide the path below\n",
    "    # Example: world = gpd.read_file('/path/to/ne_110m_admin_0_countries.shp')\n",
    "    # For convenience, try to use the 'naturalearth_lowres' from geopandas if available, else fallback to a downloaded file\n",
    "\n",
    "    try:\n",
    "        # Try the old method (may fail in geopandas >= 1.0)\n",
    "        world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    except (AttributeError, KeyError):\n",
    "        # Fallback: download and use the shapefile manually\n",
    "        # Please download from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/\n",
    "        # and set the correct path below\n",
    "        world = gpd.read_file('ne_110m_admin_0_countries.zip')\n",
    "\n",
    "    ukraine = world[world['NAME'] == 'Ukraine'] if 'NAME' in world.columns else world[world['name'] == 'Ukraine']\n",
    "\n",
    "    # Plot the outline on top of the map\n",
    "    ax = plt.gca()\n",
    "    ukraine.boundary.plot(ax=ax, edgecolor='red', linewidth=1, zorder=10)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec9fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each month in the analysis period\n",
    "for start_time_str_loop in pd.date_range(start=pd.to_datetime(start_time_str), end=pd.to_datetime(stop_time_str), freq='MS', tz='UTC'):\n",
    "    stop_time_str_loop = (start_time_str_loop + pd.offsets.MonthEnd(1)).replace(hour=23, minute=59, second=59)\n",
    "    start_time_simple_loop = pd.to_datetime(start_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    stop_time_simple_loop = pd.to_datetime(stop_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    month_year = pd.to_datetime(start_time_str_loop).strftime('%B %Y')\n",
    "\n",
    "    # Load the NOx emissions grid for this month\n",
    "    nox_grid = np.load(f'/home/omg28/nethome/Data/emissions/{start_time_simple_loop}_to_{stop_time_simple_loop}_NOx_nowar.npy')\n",
    "\n",
    "    # Sum over altitude to get total NOx flux per (lat, lon) cell\n",
    "    nox_lat_lon = np.sum(nox_grid, axis=2)\n",
    "\n",
    "    # Define zoomed-in region for the UK\n",
    "    uk_lat_min, uk_lat_max = 48, 61\n",
    "    uk_lon_min, uk_lon_max = -11, 3\n",
    "\n",
    "    # Find indices for the zoomed-in region\n",
    "    lat_mask = (lat_bins[:-1] >= uk_lat_min) & (lat_bins[:-1] <= uk_lat_max)\n",
    "    lon_mask = (lon_bins[:-1] >= uk_lon_min) & (lon_bins[:-1] <= uk_lon_max)\n",
    "\n",
    "    nox_lat_lon_zoom = nox_lat_lon[np.ix_(lat_mask, lon_mask)]\n",
    "    lat_bins_zoom = lat_bins[np.where(lat_mask)[0][0]:np.where(lat_mask)[0][-1]+2]\n",
    "    lon_bins_zoom = lon_bins[np.where(lon_mask)[0][0]:np.where(lon_mask)[0][-1]+2]\n",
    "\n",
    "    # Compute the proportion of total emissions for each (lat, lon) cell\n",
    "    total_emissions = np.sum(nox_lat_lon_zoom)\n",
    "    if total_emissions > 0:\n",
    "        nox_lat_lon_prop = nox_lat_lon_zoom / total_emissions\n",
    "    else:\n",
    "        nox_lat_lon_prop = nox_lat_lon_zoom  # all zeros\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    im = plt.imshow(\n",
    "        nox_lat_lon_prop,\n",
    "        extent=[lon_bins_zoom[0], lon_bins_zoom[-2], lat_bins_zoom[0], lat_bins_zoom[-2]],\n",
    "        origin='lower',\n",
    "        aspect='auto',\n",
    "        cmap='turbo'\n",
    "    )\n",
    "    plt.colorbar(im, label='Proportion of Total NOx Emissions')\n",
    "    plt.xlabel('Longitude (degrees)')\n",
    "    plt.ylabel('Latitude (degrees)')\n",
    "    plt.title(f'NOx Emissions Proportion Heatmap ({month_year}): UK Region')\n",
    "\n",
    "    # Draw a thin red outline around the UK\n",
    "    try:\n",
    "        world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    except (AttributeError, KeyError):\n",
    "        world = gpd.read_file('ne_110m_admin_0_countries.zip')\n",
    "    uk = world[world['NAME'] == 'United Kingdom'] if 'NAME' in world.columns else world[world['name'] == 'United Kingdom']\n",
    "    ax = plt.gca()\n",
    "    uk.boundary.plot(ax=ax, edgecolor='red', linewidth=1, zorder=10)\n",
    "\n",
    "    plt.xlim(uk_lon_min, uk_lon_max)\n",
    "    plt.ylim(uk_lat_min, uk_lat_max)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb86082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Calculate and plot the difference in NOx emissions between conflict and no-conflict scenarios\n",
    "\n",
    "# Initialize arrays to store differences for each month\n",
    "monthly_differences = []\n",
    "monthly_nowar_totals = []\n",
    "month_labels = []\n",
    "\n",
    "# Create custom binary colormap\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Define colors: blue for decrease, white for no change, yellow for increase\n",
    "colors = ['blue', 'white', 'yellow']\n",
    "n_bins = 100\n",
    "cmap_binary = ListedColormap(colors)\n",
    "\n",
    "# Loop over each month in the analysis period\n",
    "for start_time_str_loop in pd.date_range(start=pd.to_datetime(start_time_str), end=pd.to_datetime(stop_time_str), freq='MS', tz='UTC'):\n",
    "    stop_time_str_loop = (start_time_str_loop + pd.offsets.MonthEnd(1)).replace(hour=23, minute=59, second=59)\n",
    "    start_time_simple_loop = pd.to_datetime(start_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    stop_time_simple_loop = pd.to_datetime(stop_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    month_year = pd.to_datetime(start_time_str_loop).strftime('%B %Y')\n",
    "    \n",
    "    try:\n",
    "        # Load both conflict and no-conflict NOx emissions grids\n",
    "        nox_grid_war = np.load(f'/home/omg28/nethome/Data/emissions/{start_time_simple_loop}_to_{stop_time_simple_loop}_NOx_war.npy')\n",
    "        nox_grid_nowar = np.load(f'/home/omg28/nethome/Data/emissions/{start_time_simple_loop}_to_{stop_time_simple_loop}_NOx_nowar.npy')\n",
    "        \n",
    "        # Calculate the difference (war scenario - no war scenario)\n",
    "        nox_difference = nox_grid_war - nox_grid_nowar\n",
    "        \n",
    "        # Sum over altitude to get total difference per (lat, lon) cell\n",
    "        nox_diff_lat_lon = np.sum(nox_difference, axis=2)\n",
    "        nox_nowar_lat_lon = np.sum(nox_grid_nowar, axis=2)\n",
    "        \n",
    "        monthly_differences.append(nox_diff_lat_lon)\n",
    "        monthly_nowar_totals.append(nox_nowar_lat_lon)\n",
    "        month_labels.append(month_year)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Emissions data not found for {month_year}\")\n",
    "        continue\n",
    "\n",
    "# Calculate annual average difference with binary colormap\n",
    "if monthly_differences:\n",
    "    annual_avg_difference = np.mean(monthly_differences, axis=0)\n",
    "    annual_avg_nowar = np.mean(monthly_nowar_totals, axis=0)\n",
    "    \n",
    "    # Create binary classification for annual average\n",
    "    binary_annual = np.zeros_like(annual_avg_difference)\n",
    "    binary_annual[annual_avg_difference > 0] = 1   # Increase\n",
    "    binary_annual[annual_avg_difference < 0] = -1  # Decrease\n",
    "    \n",
    "    # Calculate zonally averaged relative change (percentage)\n",
    "    zonal_avg_diff = np.mean(annual_avg_difference, axis=1)\n",
    "    zonal_avg_nowar = np.mean(annual_avg_nowar, axis=1)\n",
    "    \n",
    "    # Calculate relative change as percentage, avoiding division by zero\n",
    "    zonal_relative_change = np.zeros_like(zonal_avg_diff)\n",
    "    nonzero_mask = zonal_avg_nowar != 0\n",
    "    zonal_relative_change[nonzero_mask] = (zonal_avg_diff[nonzero_mask] / zonal_avg_nowar[nonzero_mask]) * 100\n",
    "    \n",
    "    # Calculate total annual statistics\n",
    "    total_increase = np.sum(annual_avg_difference[annual_avg_difference > 0])\n",
    "    total_decrease = np.sum(annual_avg_difference[annual_avg_difference < 0])\n",
    "    net_change = np.sum(annual_avg_difference)\n",
    "    \n",
    "    # Calculate annual totals for percentage\n",
    "    annual_war_total = np.sum([np.sum(np.load(f'/home/omg28/nethome/Data/emissions/{pd.to_datetime(start_loop).strftime(\"%Y-%m-%d\")}_to_{(pd.to_datetime(start_loop) + pd.offsets.MonthEnd(1)).strftime(\"%Y-%m-%d\")}_NOx_war.npy')) \n",
    "                              for start_loop in pd.date_range(start=pd.to_datetime(start_time_str), end=pd.to_datetime(stop_time_str), freq='MS', tz='UTC')])\n",
    "    annual_nowar_total = np.sum([np.sum(np.load(f'/home/omg28/nethome/Data/emissions/{pd.to_datetime(start_loop).strftime(\"%Y-%m-%d\")}_to_{(pd.to_datetime(start_loop) + pd.offsets.MonthEnd(1)).strftime(\"%Y-%m-%d\")}_NOx_nowar.npy')) \n",
    "                                for start_loop in pd.date_range(start=pd.to_datetime(start_time_str), end=pd.to_datetime(stop_time_str), freq='MS', tz='UTC')])\n",
    "    \n",
    "    print(f\"Annual {analysis_year} NOx Emissions Statistics:\")\n",
    "    print(f\"  Total increase in emissions: {total_increase:.2f} kg/year\")\n",
    "    print(f\"  Total decrease in emissions: {total_decrease:.2f} kg/year\")\n",
    "    print(f\"  Net change in emissions: {net_change:.2f} kg/year\")\n",
    "    print(f\"  Percentage change: {(net_change/annual_nowar_total*100):.4f}%\")\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(14, 7))\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[4, 1], hspace=0.05, wspace=0.05)\n",
    "    \n",
    "    # Main map subplot\n",
    "    ax_map = fig.add_subplot(gs[0])\n",
    "    im = ax_map.imshow(\n",
    "        binary_annual,\n",
    "        extent=[-180, 180, lat_bins[0], lat_bins[-2]],\n",
    "        origin='lower',\n",
    "        aspect='auto',\n",
    "        cmap=cmap_binary,\n",
    "        vmin=-1,\n",
    "        vmax=1\n",
    "    )\n",
    "    \n",
    "    # Set limits\n",
    "    ax_map.set_xlim(-180, 180)\n",
    "    ax_map.set_ylim(-60, lat_bins[-2])\n",
    "    \n",
    "    ax_map.set_xlabel('Longitude (degrees)', fontsize=12)\n",
    "    ax_map.set_ylabel('Latitude (degrees)', fontsize=12)\n",
    "    ax_map.set_title(f'Annual Average Binary Change in NOx Emissions Due to Conflict ({analysis_year})', fontsize=14)\n",
    "    ax_map.tick_params(axis='both', labelsize=10)\n",
    "    \n",
    "    # Overlay country boundaries with light colors\n",
    "    try:\n",
    "        world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    except (AttributeError, KeyError):\n",
    "        world = gpd.read_file('ne_110m_admin_0_countries.zip')\n",
    "    \n",
    "    # Add light country boundaries\n",
    "    world.boundary.plot(ax=ax_map, edgecolor='lightgray', linewidth=0.5, alpha=0.7, zorder=5)\n",
    "    \n",
    "    # Highlight conflict countries with darker boundaries\n",
    "    conflict_zones = world[world['NAME'].isin(conflict_countries)] if 'NAME' in world.columns else world[world['name'].isin(conflict_countries)]\n",
    "    conflict_zones.boundary.plot(ax=ax_map, edgecolor='black', linewidth=1, zorder=10)\n",
    "    \n",
    "    # Create colorbar overlaid on the map (top right corner)\n",
    "    cbar = plt.colorbar(im, ax=ax_map, shrink=0.6, aspect=15, pad=0.02, \n",
    "                       ticks=[-1, 0, 1])\n",
    "    cbar.set_ticklabels(['Decrease', 'No Change', 'Increase'])\n",
    "    cbar.ax.tick_params(labelsize=9)\n",
    "    cbar.set_label('Change in NOx Emissions (War vs No War)', fontsize=9)\n",
    "    \n",
    "    # Zonal average subplot\n",
    "    ax_zonal = fig.add_subplot(gs[1], sharey=ax_map)  # share y-axis with main map\n",
    "    lat_centers = (lat_bins[:-1] + lat_bins[1:]) / 2\n",
    "    ax_zonal.plot(zonal_relative_change, lat_centers, 'k-', linewidth=2)\n",
    "    ax_zonal.axvline(x=0, color='gray', linestyle='--', alpha=0.7)\n",
    "    ax_zonal.set_ylim(ax_map.get_ylim())  # ensure same y-limits\n",
    "    ax_zonal.set_xlabel('Zonal Avg\\nChange (%)', fontsize=10)\n",
    "    ax_zonal.set_title('Latitudinal\\nProfile', fontsize=11)\n",
    "    ax_zonal.tick_params(axis='both', labelsize=9)\n",
    "    ax_zonal.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove y-axis labels for zonal plot since they're shared with main plot\n",
    "    plt.setp(ax_zonal.get_yticklabels(), visible=False)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d51161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate meridional shift in NOx emissions distribution for 2023\n",
    "# This analysis examines how the latitudinal distribution of emissions changes due to conflict\n",
    "\n",
    "# Initialize arrays to store meridional distributions\n",
    "monthly_nowar_lat = []\n",
    "monthly_war_lat = []\n",
    "month_labels = []\n",
    "\n",
    "# Loop over each month in the analysis period\n",
    "for start_time_str_loop in pd.date_range(start=pd.to_datetime(start_time_str), end=pd.to_datetime(stop_time_str), freq='MS', tz='UTC'):\n",
    "    stop_time_str_loop = (start_time_str_loop + pd.offsets.MonthEnd(1)).replace(hour=23, minute=59, second=59)\n",
    "    start_time_simple_loop = pd.to_datetime(start_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    stop_time_simple_loop = pd.to_datetime(stop_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    month_year = pd.to_datetime(start_time_str_loop).strftime('%B %Y')\n",
    "    \n",
    "    try:\n",
    "        # Load both conflict and no-conflict NOx emissions grids\n",
    "        nox_grid_war = np.load(f'/home/omg28/nethome/Data/emissions/{start_time_simple_loop}_to_{stop_time_simple_loop}_NOx_war.npy')\n",
    "        nox_grid_nowar = np.load(f'/home/omg28/nethome/Data/emissions/{start_time_simple_loop}_to_{stop_time_simple_loop}_NOx_nowar.npy')\n",
    "        \n",
    "        # Sum over longitude and altitude to get meridional (latitude) distribution\n",
    "        nox_nowar_lat = np.sum(nox_grid_nowar, axis=(1, 2))  # Sum over lon and alt\n",
    "        nox_war_lat = np.sum(nox_grid_war, axis=(1, 2))      # Sum over lon and alt\n",
    "        \n",
    "        monthly_nowar_lat.append(nox_nowar_lat)\n",
    "        monthly_war_lat.append(nox_war_lat)\n",
    "        month_labels.append(month_year)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Emissions data not found for {month_year}\")\n",
    "        continue\n",
    "\n",
    "# Calculate annual average meridional distributions\n",
    "if monthly_nowar_lat and monthly_war_lat:\n",
    "    annual_nowar_lat = np.mean(monthly_nowar_lat, axis=0)\n",
    "    annual_war_lat = np.mean(monthly_war_lat, axis=0)\n",
    "    \n",
    "    # Calculate latitude centers for each bin\n",
    "    lat_centers = (lat_bins[:-1] + lat_bins[1:]) / 2\n",
    "    \n",
    "    # Calculate the shift in distribution\n",
    "    shift_lat = annual_war_lat - annual_nowar_lat\n",
    "    \n",
    "    # Calculate relative change (percentage)\n",
    "    relative_change = np.zeros_like(shift_lat)\n",
    "    nonzero_mask = annual_nowar_lat > 0\n",
    "    relative_change[nonzero_mask] = (shift_lat[nonzero_mask] / annual_nowar_lat[nonzero_mask]) * 100\n",
    "    \n",
    "    # Plot the meridional distributions\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Subplot 1: Absolute distributions\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(lat_centers, annual_nowar_lat, 'b-', linewidth=2, label='No War Scenario')\n",
    "    plt.plot(lat_centers, annual_war_lat, 'r-', linewidth=2, label='War Scenario')\n",
    "    plt.xlabel('Latitude (degrees)')\n",
    "    plt.ylabel('NOx Emissions (kg/year)')\n",
    "    plt.title('Annual Average Meridional Distribution of NOx Emissions')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Absolute difference\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(lat_centers, shift_lat, 'g-', linewidth=2)\n",
    "    plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('Latitude (degrees)')\n",
    "    plt.ylabel('Change in NOx Emissions (kg/year)')\n",
    "    plt.title('Absolute Change in Meridional Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 3: Relative change (percentage)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(lat_centers, relative_change, 'purple', linewidth=2)\n",
    "    plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('Latitude (degrees)')\n",
    "    plt.ylabel('Relative Change (%)')\n",
    "    plt.title('Relative Change in Meridional Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 4: Cumulative distributions (normalized)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    cum_nowar = np.cumsum(annual_nowar_lat) / np.sum(annual_nowar_lat)\n",
    "    cum_war = np.cumsum(annual_war_lat) / np.sum(annual_war_lat)\n",
    "    plt.plot(lat_centers, cum_nowar, 'b-', linewidth=2, label='No War (Cumulative)')\n",
    "    plt.plot(lat_centers, cum_war, 'r-', linewidth=2, label='War (Cumulative)')\n",
    "    plt.xlabel('Latitude (degrees)')\n",
    "    plt.ylabel('Cumulative Fraction of Total Emissions')\n",
    "    plt.title('Cumulative Meridional Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate key statistics\n",
    "    total_nowar = np.sum(annual_nowar_lat)\n",
    "    total_war = np.sum(annual_war_lat)\n",
    "    \n",
    "    # Calculate centroid shift\n",
    "    centroid_nowar = np.sum(lat_centers * annual_nowar_lat) / total_nowar\n",
    "    centroid_war = np.sum(lat_centers * annual_war_lat) / total_war\n",
    "    centroid_shift = centroid_war - centroid_nowar\n",
    "    \n",
    "    # Calculate emissions by latitude bands\n",
    "    northern_lats = lat_centers >= 30\n",
    "    temperate_lats = (lat_centers >= 0) & (lat_centers < 30)\n",
    "    southern_lats = lat_centers < 0\n",
    "    \n",
    "    northern_nowar = np.sum(annual_nowar_lat[northern_lats])\n",
    "    northern_war = np.sum(annual_war_lat[northern_lats])\n",
    "    \n",
    "    temperate_nowar = np.sum(annual_nowar_lat[temperate_lats])\n",
    "    temperate_war = np.sum(annual_war_lat[temperate_lats])\n",
    "    \n",
    "    southern_nowar = np.sum(annual_nowar_lat[southern_lats])\n",
    "    southern_war = np.sum(annual_war_lat[southern_lats])\n",
    "    \n",
    "    print(f\"\\n=== Meridional Shift Analysis for {analysis_year} ===\")\n",
    "    print(f\"Emission centroid shift: {centroid_shift:.3f}° latitude\")\n",
    "    print(f\"No war centroid: {centroid_nowar:.2f}°N\")\n",
    "    print(f\"War centroid: {centroid_war:.2f}°N\")\n",
    "    \n",
    "    print(f\"\\nRegional Distribution Changes:\")\n",
    "    print(f\"Northern latitudes (≥30°N):\")\n",
    "    print(f\"  No war: {northern_nowar:.0f} kg/year ({northern_nowar/total_nowar*100:.1f}%)\")\n",
    "    print(f\"  War: {northern_war:.0f} kg/year ({northern_war/total_war*100:.1f}%)\")\n",
    "    print(f\"  Change: {northern_war-northern_nowar:.0f} kg/year ({(northern_war-northern_nowar)/northern_nowar*100:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nTemperate latitudes (0°-30°N):\")\n",
    "    print(f\"  No war: {temperate_nowar:.0f} kg/year ({temperate_nowar/total_nowar*100:.1f}%)\")\n",
    "    print(f\"  War: {temperate_war:.0f} kg/year ({temperate_war/total_war*100:.1f}%)\")\n",
    "    print(f\"  Change: {temperate_war-temperate_nowar:.0f} kg/year ({(temperate_war-temperate_nowar)/temperate_nowar*100:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nSouthern latitudes (<0°):\")\n",
    "    print(f\"  No war: {southern_nowar:.0f} kg/year ({southern_nowar/total_nowar*100:.1f}%)\")\n",
    "    print(f\"  War: {southern_war:.0f} kg/year ({southern_war/total_war*100:.1f}%)\")\n",
    "    print(f\"  Change: {southern_war-southern_nowar:.0f} kg/year ({(southern_war-southern_nowar)/southern_nowar*100:.2f}%)\")\n",
    "    \n",
    "    # Find latitude of maximum change\n",
    "    max_increase_lat = lat_centers[np.argmax(shift_lat)]\n",
    "    max_decrease_lat = lat_centers[np.argmin(shift_lat)]\n",
    "    \n",
    "    print(f\"\\nLatitude of maximum increase: {max_increase_lat:.1f}°N ({np.max(shift_lat):.0f} kg/year)\")\n",
    "    print(f\"Latitude of maximum decrease: {max_decrease_lat:.1f}°N ({np.min(shift_lat):.0f} kg/year)\")\n",
    "\n",
    "else:\n",
    "    print(\"No emissions data found for analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
