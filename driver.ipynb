{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a396014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_day import download_day\n",
    "from load_saved_fd4 import load_saved_fd4\n",
    "from scrape_aircraft_list import scrape_aircraft_list\n",
    "from get_perf_model_typecodes import get_perf_model_typecodes \n",
    "from match_icao_model import match_icao_model\n",
    "from process_airport_list import process_airport_list\n",
    "from generate_flightpath import generate_flightpath\n",
    "from plot_flightpaths import plot_flightpaths\n",
    "from get_engine_data import get_engine_data\n",
    "from perf_model_powerplant_parser import perf_model_powerplant_parser\n",
    "from match_engine_to_emissions_db import match_engine_to_emissions_db\n",
    "from process_month import process_month\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from time import sleep\n",
    "from geopy import distance\n",
    "from icet import icet\n",
    "from bffm2 import bffm2\n",
    "%matplotlib inline\n",
    "\n",
    "# User Inputs:\n",
    "start_time_str       = '2023-01-01T00:00:00Z'\n",
    "stop_time_str        = '2023-12-31T23:59:59Z'\n",
    "query_limit          = 15e4\n",
    "send_notification    = True\n",
    "make_plot            = True\n",
    "output_dir           = \"/scratch/omg28/Data/\"\n",
    "\n",
    "# Convert start and stop times to datetime objects\n",
    "start_time_simple = pd.to_datetime(start_time_str).strftime(\"%Y-%m-%d\")\n",
    "stop_time_simple = pd.to_datetime(stop_time_str).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e008886",
   "metadata": {},
   "source": [
    "# Section 1. Download flight information from Opensky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278aa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download data from OpenSky history database\n",
    "# WARNING: This may take a long time to run and will use a lot of disk space. Recommend running this in a .py script to avoid recurring memory issues found in Jupyter notebooks.\n",
    "# download_day(start_time_str, stop_time_str, query_limit, send_notification, make_plot, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bcd15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved day of data from a .pkl file to a pandas dataframe\n",
    "# loaded_day = load_saved_fd4(stop_time_str, output_dir, query_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c13875",
   "metadata": {},
   "source": [
    "# Section 2. Scrape the EUCONTROL database for aircraft flight performance information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# scrape the list of aircraft with performance models from the EUCONTROL website\n",
    "perf_model_typecodes = get_perf_model_typecodes()\n",
    "## WARNING: THIS CAN GET YOUR IP ADDRESS BLOCKED IF YOU RUN IT REPEATEDLY. IT IS A WEB SCRAPER.\n",
    "aircraft_list = scrape_aircraft_list(perf_model_typecodes)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the helicopters from the aircraft performance database list\n",
    "perf_models = pd.read_csv(\"aircraft_performance_table.csv\")\n",
    "perf_models = perf_models[~perf_models['type'].str.contains('H')]\n",
    "perf_models.to_csv(\"aircraft_performance_table_filtered.csv\", index=False)\n",
    "pd.to_pickle(perf_models, \"aircraft_performance_table_filtered.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e861cc1",
   "metadata": {},
   "source": [
    "# Section 3. Process the aircraft performance and flight data, retaining only flights we have takeoff, landing, plane type, and aircraft performance model information about. Further, drop all flights with aircraft codes that do not correspond to a code in the ourairports registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1246e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the loaded time period of data. - 26s\n",
    "\n",
    "# - matches 24 bit transponder codes from Feb 2025 aircraft metadata database with the ICAO typecode\n",
    "\n",
    "# - removes all flights for which the transponder code does not have a corresponding ICAO typecode\n",
    "\n",
    "# - removes all flights for which the takeoff OR landing airport is unknown\n",
    "\n",
    "for start_time_str_loop in pd.date_range(start=pd.to_datetime(start_time_str), end=pd.to_datetime(stop_time_str), freq='MS', tz='UTC'):\n",
    "    # Set end date to last day of month at 23:59:59 UTC\n",
    "    stop_time_str_loop = (start_time_str_loop + pd.offsets.MonthEnd(1)).replace(hour=23, minute=59, second=59)\n",
    "    start_time_simple_loop = pd.to_datetime(start_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    stop_time_simple_loop = pd.to_datetime(stop_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    print(f\"Processing data from {start_time_str_loop} to {stop_time_str_loop}\")\n",
    "\n",
    "    flights_with_typecode = match_icao_model(str(start_time_str_loop), str(stop_time_str_loop), query_limit, \n",
    "                                            aircraft_db_path = \"/scratch/omg28/Data/aircraftdb/aircraft-database-complete-2025-02.csv\",\n",
    "                                            flight_db_path = \"/scratch/omg28/Data/no_track2023/\",\n",
    "                                            output_dir = \"/scratch/omg28/Data/aircraftdb/\")\n",
    "\n",
    "    # Load in the list of ICAO typecodes that have a performance model - 7.5s\n",
    "    performance_model_typecodes = pd.read_pickle('/scratch/omg28/Data/aircraftdb/performance_models_typecodes.pkl')\n",
    "\n",
    "    # load in the list of all flights with typecode and takeoff/landing airport - uncomment this line to load the data from a .pkl file\n",
    "    # instead of running the match_icao_model function\n",
    "    # flights_with_typecode = pd.read_pickle(f'/scratch/omg28/Data/aircraftdb/{start_time_simple_loop}_to_{stop_time_simple_loop}_{int(query_limit)}_typecodes_added.pkl')\n",
    "\n",
    "    # get info before the merge\n",
    "    print(\"Before merging:\")\n",
    "    print(flights_with_typecode.info())\n",
    "\n",
    "    # inner join the loaded year of data with the scraped aircraft list\n",
    "    # - this will remove all flights for which the ICAO typecode does not have a corresponding performance model\n",
    "    flights_with_perf_typecode = pd.merge(flights_with_typecode, performance_model_typecodes, how='inner', on = 'typecode')\n",
    "\n",
    "    print(\"After merging:\")\n",
    "    print(flights_with_perf_typecode.info())\n",
    "\n",
    "    # print a list of typecodes in complete_flights_perf_model that do not have a performance model\n",
    "    missing_perf_models = flights_with_typecode[~flights_with_typecode['typecode'].isin(performance_model_typecodes['typecode'])]\n",
    "    print(\"Aircraft models with observed flights, but no performance model: \" + str(len(missing_perf_models['typecode'].unique())))\n",
    "    print(\"total flights in 2024 with TOLD and typecode, but no performance model: \" + str(len(missing_perf_models)))\n",
    "\n",
    "    # print a list of typecodes in performance_model_typecodes that do not correspond to a flight\n",
    "    missing_flights = performance_model_typecodes[~performance_model_typecodes['typecode'].isin(flights_with_typecode['typecode'])]\n",
    "    print(\"Number of performance models with no flights: \" + str(len(missing_flights['typecode'].unique())))\n",
    "    print(\"Number of flights with TOLD, typecode, and performance model: \" + str(len(flights_with_perf_typecode)))\n",
    "\n",
    "    # count all flights starting and ending at the same airport\n",
    "    num_flights_no_dist = len(flights_with_perf_typecode[flights_with_perf_typecode['estdepartureairport'] == flights_with_perf_typecode['estarrivalairport']])\n",
    "    print(\"Number of flights with no distance: \" + str(num_flights_no_dist))\n",
    "\n",
    "    # remove all flights starting or ending at the same airport\n",
    "    flights_with_perf_typecode_dist = flights_with_perf_typecode[flights_with_perf_typecode['estdepartureairport'] != flights_with_perf_typecode['estarrivalairport']]\n",
    "    print(\"Number of flights with TOLD, typecode, performance model, and nonzero distance: \" + str(len(flights_with_perf_typecode_dist)))\n",
    "\n",
    "    # load the takeoff and landing airport location information to the dataframe. 18s\n",
    "    all_airports = process_airport_list()\n",
    "\n",
    "    # add the estimated departure airport location information to the dataframe\n",
    "    # - this will remove all flights for which the departure airport is unknown\n",
    "    flights_with_perf_typecode_dist_dep = pd.merge(flights_with_perf_typecode_dist, all_airports, how='inner', left_on = 'estdepartureairport', right_on = 'ident')\n",
    "    flights_with_perf_typecode_dist_dep.rename(columns = {'latitude_deg': 'estdeparturelat', 'longitude_deg': 'estdeparturelong', 'elevation_ft': 'estdeparturealt_ft'}, inplace=True)\n",
    "    flights_with_perf_typecode_dist_dep.drop(columns = ['ident', 'gps_code'], inplace=True)\n",
    "\n",
    "    # count the number of flights with no departure airport in the database\n",
    "    print(\"Number of flights with no departure airport location info in database: \" + str(len(flights_with_perf_typecode_dist) - len(flights_with_perf_typecode_dist_dep)))\n",
    "\n",
    "    # add the estimated arrival airport location information to the dataframe\n",
    "    all_flights_filtered = pd.merge(flights_with_perf_typecode_dist_dep, all_airports, how='inner', left_on = 'estarrivalairport', right_on = 'ident')\n",
    "    all_flights_filtered.rename(columns = {'latitude_deg': 'estarrivallat', 'longitude_deg': 'estarrivallong', 'elevation_ft': 'estarrivalalt_ft'}, inplace=True)\n",
    "    all_flights_filtered.drop(columns = ['ident', 'gps_code', 'id_x', 'id_y'], inplace=True)\n",
    "    \n",
    "    # save the result to a pickle file\n",
    "    output_dir = os.path.expanduser(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filename = os.path.join(output_dir, f\"aircraftdb/{start_time_simple_loop}_to_{stop_time_simple_loop}_{int(query_limit)}_typecodes_airports_added.pkl\")\n",
    "    all_flights_filtered.to_pickle(filename)\n",
    "\n",
    "    # count the number of flights with no arrival airport in the database\n",
    "    print(\"Number of flights with no arrival airport location info in database: \" + str(len(flights_with_perf_typecode_dist_dep) - len(all_flights_filtered)))\n",
    "    print(\"Number of flights with TOLD, typecode, performance model, distance, and departure/arrival airport info: \" + str(len(all_flights_filtered)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8a321c",
   "metadata": {},
   "source": [
    "# Section 4. Using the great circle distance, calculate the distance between the airport takeoff and landing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd02fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the great circle distance between the departure and arrival airports - 4 min\n",
    "for start_time_str_loop in pd.date_range(start=pd.to_datetime(start_time_str), end=pd.to_datetime(stop_time_str), freq='MS', tz='UTC'):\n",
    "    # Set end date to last day of month at 23:59:59 UTC\n",
    "    stop_time_str_loop = (start_time_str_loop + pd.offsets.MonthEnd(1)).replace(hour=23, minute=59, second=59)\n",
    "    start_time_simple_loop = pd.to_datetime(start_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    stop_time_simple_loop = pd.to_datetime(stop_time_str_loop).strftime(\"%Y-%m-%d\")\n",
    "    # calculare great circle distance between the departure and arrival airports\n",
    "    all_flights_filtered = pd.read_pickle(f'/scratch/omg28/Data/aircraftdb/{start_time_simple_loop}_to_{stop_time_simple_loop}_{int(query_limit)}_typecodes_airports_added.pkl')\n",
    "    all_flights_filtered['gc_km'] = all_flights_filtered.apply(lambda x: distance.great_circle((x['estdeparturelat'], x['estdeparturelong']), (x['estarrivallat'], x['estarrivallong'])).km, axis=1)\n",
    "    \n",
    "    # could be sped up by using a vectorized function - 1 min\n",
    "    # adjust great circle distance using eq. 6 of seymour, et al (2020)\n",
    "    all_flights_filtered['gc_FEAT_km'] = all_flights_filtered.apply(lambda x: 1.0387 * x['gc_km'] + 40.5, axis = 1)\n",
    "    pd.to_pickle(all_flights_filtered, f'{output_dir}/{start_time_simple_loop}_to_{stop_time_simple_loop}_filtered.pkl')\n",
    "    \n",
    "    # remove all flights under 200 km:\n",
    "    all_flights_filtered = all_flights_filtered[all_flights_filtered['gc_FEAT_km'] > 200]\n",
    "    print(\"Number of flights with TOLD, typecode, performance model, distance, and departure/arrival airport info over 200 km: \" + str(len(all_flights_filtered)))\n",
    "\n",
    "    # update max_distances across all months\n",
    "    if 'max_distances_all' not in locals():\n",
    "        max_distances_all = all_flights_filtered.groupby('typecode')['gc_FEAT_km'].max().reset_index()\n",
    "        max_distances_all.columns = ['typecode', 'max_distance_km']\n",
    "    else:\n",
    "        max_distances_month = all_flights_filtered.groupby('typecode')['gc_FEAT_km'].max().reset_index()\n",
    "        max_distances_month.columns = ['typecode', 'max_distance_km']\n",
    "        # merge and keep the maximum for each typecode\n",
    "        max_distances_all = pd.concat([max_distances_all, max_distances_month]).groupby('typecode', as_index=False)['max_distance_km'].max()\n",
    "\n",
    "    # save the merged dataframe to a .pkl file - 15s\n",
    "    all_flights_filtered.to_pickle(f'{output_dir}/aircraftdb/{start_time_simple}_to_{stop_time_simple}_filtered.pkl')\n",
    "    # save the max distances dataframe to a .csv file\n",
    "    max_distances_all.to_csv(f'{output_dir}/aircraftdb/{start_time_simple}_to_{stop_time_simple}_typecode_max_distances.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e303638c",
   "metadata": {},
   "source": [
    "# Section 5: parse the engine data and emissions data and add it to the aircraft performance model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a921d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data from the saved engine data .csv files for analysis. Saves this dataframe to a .pkl file\n",
    "# Setting \"remove_superseded\" to True will remove all engines that are superseded by an updated emissions model\n",
    "engine_models = get_engine_data(update_superseded=False)\n",
    "engine_models_removed = get_engine_data(update_superseded=True)\n",
    "# calculate how many engines were removed\n",
    "print(\"Number of engines that would be removed: \" + str(len(engine_models['Engine Identification'].unique()) - len(engine_models_removed['Engine Identification'].unique())))\n",
    "# print the list of the engine models that are in engine_models_kept but not in engine_models_removed\n",
    "superseded_not_replaced = engine_models[~engine_models['Engine Identification'].isin(engine_models_removed['Engine Identification'])].get('Engine Identification').unique()\n",
    "print(\"List of engines that were removed: \" + str(superseded_not_replaced))\n",
    "# POTENTIAL FIX: REMOVE ENGINES THAT HAVE BEEN SUPERSEDED BY AN UPDATED EMISSIONS MODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60bd30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the aircraft performance model data from the saved .csv file\n",
    "performance_models = pd.read_csv(\"aircraft_performance_table_filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff54bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts the powerplant data from the performance model dataframe and appends it to new columns in the performance model dataframe\n",
    "allowed_manufacturers = engine_models['Manufacturer'].unique().tolist()\n",
    "performance_models = perf_model_powerplant_parser(performance_models, coerce_manufacturer=True, allowed_manufacturers=allowed_manufacturers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f83aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12s - attempts to match the powerplants extracted from the EUCONTROL aircraft performance model with an engine model in the engine emissions database\n",
    "# reset the index of engine_models so that 'UID No' becomes a column rather than the index, otherwise the merge will not work\n",
    "engine_models.reset_index(inplace=True)\n",
    "# attempts to match the powerplants extracted from the EUCONTROL aircraft performance model with an engine model in the engine emissions database\n",
    "performance_models_matched = match_engine_to_emissions_db(performance_models, engine_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from all rows without a matched engine model\n",
    "performance_models_matched = performance_models_matched.dropna(subset=['matched_engine_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaaa309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of performance models whose engines match superseded engines\n",
    "print(\"Number of performance models with engines that are superseded: \" + str(len(performance_models_matched[performance_models_matched['matched_engine_id'].isin(superseded_not_replaced)])))\n",
    "print(\"Number of performance models with engines that are NOT superseded: \" + str(len(performance_models_matched[~performance_models_matched['matched_engine_id'].isin(superseded_not_replaced)])))\n",
    "print(\"aircraft performance models with engines that are superseded: \\n\" + str(performance_models_matched[performance_models_matched['matched_engine_id'].isin(superseded_not_replaced)].get('typecode')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28deb160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the aircraft performance model data with the engine emissions database data\n",
    "performance_and_emissions_model = pd.merge(performance_models_matched, engine_models, how='inner', left_on='matched_engine_uid_no', right_on='UID No')\n",
    "pd.to_pickle(performance_and_emissions_model, \"performance_and_emissions_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c61dadc",
   "metadata": {},
   "source": [
    "# Section 6: generate and visualize flightpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance_data=pd.read_pickle('aircraft_performance_data_table.pkl')\n",
    "performance_and_emissions_model = pd.read_pickle('performance_and_emissions_model.pkl')\n",
    "aircraft_typecode = 'A320'\n",
    "flightpath = generate_flightpath(aircraft_typecode, 500, performance_and_emissions_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Sum durations from the flightpath dictionary for each phase\n",
    "climb_time = (flightpath['climb'].get('t_climb_0_5', 0) +\n",
    "              flightpath['climb'].get('t_climb_5_10', 0) +\n",
    "              flightpath['climb'].get('t_climb_10_15', 0) +\n",
    "              flightpath['climb'].get('t_climb_15_24', 0) +\n",
    "              flightpath['climb'].get('t_climb_ceil', 0))\n",
    "\n",
    "descent_time = (flightpath['descent'].get('t_descent_5_0', 0) +\n",
    "                flightpath['descent'].get('t_descent_10_5', 0) +\n",
    "                flightpath['descent'].get('t_descent_15_10', 0) +\n",
    "                flightpath['descent'].get('t_descent_24_15', 0) +\n",
    "                flightpath['descent'].get('t_descent_ceil', 0))\n",
    "\n",
    "cruise_time = flightpath['cruise'].get('t_cruise', 0)\n",
    "\n",
    "# Prepare data for the barchart\n",
    "phases = ['Climb', 'Cruise', 'Descent']\n",
    "durations = [climb_time, cruise_time, descent_time]\n",
    "\n",
    "# Optionally convert seconds to minutes (or leave in seconds)\n",
    "# durations = [t / 60.0 for t in durations]\n",
    "\n",
    "plt.figure()\n",
    "bars = plt.bar(phases, durations, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.xlabel('Flight Phases')\n",
    "plt.ylabel('Duration (seconds)')\n",
    "plt.title(f'Duration of Each Flight Leg ({aircraft_typecode})')\n",
    "\n",
    "# Annotate bars with duration values\n",
    "for bar, duration in zip(bars, durations):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height,\n",
    "             f'{duration:.0f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016bae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break down the climb phases\n",
    "climb_labels = ['Climb 0-5', 'Climb 5-10', 'Climb 10-15', 'Climb 15-24', 'Climb Ceil']\n",
    "climb_times = [\n",
    "    flightpath['climb'].get('t_climb_0_5', 0),\n",
    "    flightpath['climb'].get('t_climb_5_10', 0),\n",
    "    flightpath['climb'].get('t_climb_10_15', 0),\n",
    "    flightpath['climb'].get('t_climb_15_24', 0),\n",
    "    flightpath['climb'].get('t_climb_ceil', 0)\n",
    "]\n",
    "\n",
    "# Break down the descent phases\n",
    "descent_labels = ['Descent 5-0', 'Descent 10-5', 'Descent 15-10', 'Descent 24-15', 'Descent Ceil']\n",
    "descent_times = [\n",
    "    flightpath['descent'].get('t_descent_5_0', 0),\n",
    "    flightpath['descent'].get('t_descent_10_5', 0),\n",
    "    flightpath['descent'].get('t_descent_15_10', 0),\n",
    "    flightpath['descent'].get('t_descent_24_15', 0),\n",
    "    flightpath['descent'].get('t_descent_ceil', 0)\n",
    "]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Climb segments chart\n",
    "plt.subplot(1, 2, 1)\n",
    "bars_climb = plt.bar(climb_labels, climb_times, color='skyblue')\n",
    "plt.xlabel('Climb Segments')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title(f'Climb Phase Breakdown ({aircraft_typecode})')\n",
    "for bar, t in zip(bars_climb, climb_times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{t:.1f}', ha='center', va='bottom')\n",
    "\n",
    "# Descent segments chart\n",
    "plt.subplot(1, 2, 2)\n",
    "bars_descent = plt.bar(descent_labels, descent_times, color='salmon')\n",
    "plt.xlabel('Descent Segments')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title(f'Descent Phase Breakdown ({aircraft_typecode})')\n",
    "for bar, t in zip(bars_descent, descent_times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{t:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ef7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Overall phase distances (in meters)\n",
    "climb_distance = (np.atleast_1d(flightpath['climb'].get('s_climb_0_5', 0))[0] +\n",
    "                  np.atleast_1d(flightpath['climb'].get('s_climb_5_10', 0))[0] +\n",
    "                  np.atleast_1d(flightpath['climb'].get('s_climb_10_15', 0))[0] +\n",
    "                  np.atleast_1d(flightpath['climb'].get('s_climb_15_24', 0))[0] +\n",
    "                  np.atleast_1d(flightpath['climb'].get('s_climb_ceil', 0))[0])\n",
    "\n",
    "cruise_distance = np.atleast_1d(flightpath['cruise'].get('s_cruise', 0))[0]\n",
    "\n",
    "descent_distance = (np.atleast_1d(flightpath['descent'].get('s_descent_5_0', 0))[0] +\n",
    "                    np.atleast_1d(flightpath['descent'].get('s_descent_10_5', 0))[0] +\n",
    "                    np.atleast_1d(flightpath['descent'].get('s_descent_15_10', 0))[0] +\n",
    "                    np.atleast_1d(flightpath['descent'].get('s_descent_24_15', 0))[0] +\n",
    "                    np.atleast_1d(flightpath['descent'].get('s_descent_ceil', 0))[0])\n",
    "\n",
    "phases = ['Climb', 'Cruise', 'Descent']\n",
    "distances = [climb_distance, cruise_distance, descent_distance]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "overall_bars = plt.bar(phases, distances, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.xlabel('Flight Phases')\n",
    "plt.ylabel('Distance (meters)')\n",
    "plt.title(f'Distance Covered in Each Flight Phase ({aircraft_typecode})')\n",
    "\n",
    "# Annotate overall bars with distance values\n",
    "for bar, dist in zip(overall_bars, distances):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{dist:,.0f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Breakdown of distances for each segment\n",
    "\n",
    "# For Climb Phase\n",
    "climb_keys = ['s_climb_0_5', 's_climb_5_10', 's_climb_10_15', 's_climb_15_24', 's_climb_ceil']\n",
    "climb_distances = []\n",
    "for key in climb_keys:\n",
    "    value = flightpath['climb'].get(key, np.array([0]))\n",
    "    # value is expected to be a numpy array; extract its first element if so\n",
    "    if isinstance(value, np.ndarray):\n",
    "        climb_distances.append(value[0])\n",
    "    else:\n",
    "        climb_distances.append(value)\n",
    "\n",
    "# For Descent Phase\n",
    "descent_keys = ['s_descent_5_0', 's_descent_10_5', 's_descent_15_10', 's_descent_24_15', 's_descent_ceil']\n",
    "descent_distances = []\n",
    "for key in descent_keys:\n",
    "    value = flightpath['descent'].get(key, np.array([0]))\n",
    "    if isinstance(value, np.ndarray):\n",
    "        descent_distances.append(value[0])\n",
    "    else:\n",
    "        descent_distances.append(value)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Climb segments chart\n",
    "plt.subplot(1, 2, 1)\n",
    "bars_climb = plt.bar(climb_labels, climb_distances, color='skyblue')\n",
    "plt.xlabel('Climb Segments')\n",
    "plt.ylabel('Distance (meters)')\n",
    "plt.title(f'Climb Phase Distance Breakdown ({aircraft_typecode})')\n",
    "for bar, d in zip(bars_climb, climb_distances):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{d:,.0f}', ha='center', va='bottom')\n",
    "\n",
    "# Descent segments chart\n",
    "plt.subplot(1, 2, 2)\n",
    "bars_descent = plt.bar(descent_labels, descent_distances, color='salmon')\n",
    "plt.xlabel('Descent Segments')\n",
    "plt.ylabel('Distance (meters)')\n",
    "plt.title(f'Descent Phase Distance Breakdown ({aircraft_typecode})')\n",
    "for bar, d in zip(bars_descent, descent_distances):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{d:,.0f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49eca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build cumulative distance and altitude lists.\n",
    "# Starting at runway: (distance, altitude) = (0, 0)\n",
    "distances_list = [0]\n",
    "altitudes_list = [0]\n",
    "\n",
    "# Climb phase:\n",
    "# First climb segment (0-5):\n",
    "cum_distance = distances_list[-1]\n",
    "for key in flightpath:\n",
    "    for key2 in flightpath[key]:\n",
    "        if key2.startswith('s_'):\n",
    "            cum_distance += flightpath[key][key2]\n",
    "            distances_list.append(cum_distance)\n",
    "            altitudes_list.append(flightpath[key][f'{key2.replace('s_', 'h_')}_end'])\n",
    "\n",
    "# Plot the altitude profile vs. cumulative flight distance.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(distances_list, altitudes_list, marker='o', linestyle='-')\n",
    "plt.xlabel('Cumulative Flight Distance (meters)')\n",
    "plt.ylabel('Flight Altitude (meters)')\n",
    "plt.title(f'Flight Altitude vs. Flight Distance ({aircraft_typecode})')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 25 flight distances (km) with more samples at shorter distances using a non-linear (quadratic) spacing.\n",
    "max_distances = pd.read_csv(f'/scratch/omg28/Data/no_track2024/{start_time_simple}_to_{stop_time_simple}_typecode_max_distances.csv')\n",
    "max_distance_km = max_distances.loc[max_distances['typecode'] == aircraft_typecode, 'max_distance_km'].max()\n",
    "num_flightpaths = 5\n",
    "\n",
    "# Use quadratic spacing: more points at the start, fewer at the end\n",
    "# Use logarithmic spacing: more points at the start, fewer at the end\n",
    "flight_distances = np.logspace(np.log10(200), np.log10(1000), num_flightpaths)\n",
    "flightpaths = [0] * num_flightpaths\n",
    "\n",
    "\n",
    "for ii in range(num_flightpaths):\n",
    "    d_km = flight_distances[ii]\n",
    "    fp = generate_flightpath(aircraft_typecode, d_km, performance_and_emissions_model)\n",
    "    flightpaths[ii] = fp\n",
    "\n",
    "plot_flightpaths(flightpaths, aircraft_typecode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1168ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate a grid of flight profiles varying both cruising altitude and total flight distance\n",
    "max_cruise_altitude_ft = performance_and_emissions_model.loc[\n",
    "    performance_and_emissions_model['typecode'] == aircraft_typecode, 'cruise_Ceiling'\n",
    "].max() * 1e2\n",
    "\n",
    "num_altitudes = 10\n",
    "num_distances = 10\n",
    "cruise_altitudes_ft = np.linspace(18000, max_cruise_altitude_ft, num_altitudes)\n",
    "# Use quadratic spacing for grid as well\n",
    "flight_distances_km = np.logspace(np.log10(200), np.log10(max_distance_km), num_distances)\n",
    "\n",
    "flightpaths_grid = []\n",
    "labels = []\n",
    "\n",
    "for alt_ft in cruise_altitudes_ft:\n",
    "    for d_km in flight_distances_km:\n",
    "        fp = generate_flightpath(aircraft_typecode, d_km, performance_and_emissions_model, cruise_altitude_ft=alt_ft)\n",
    "        flightpaths_grid.append(fp)\n",
    "        labels.append(f\"{int(alt_ft/1000)}kft, {int(d_km)}km\")\n",
    "\n",
    "# Example: plot total NOx emissions for each profile\n",
    "total_nox = []\n",
    "for fp in flightpaths_grid:\n",
    "    nox = 0\n",
    "    for k in ['NOx_climb_0_5', 'NOx_climb_5_10', 'NOx_climb_10_15', 'NOx_climb_15_24', 'NOx_climb_ceil']:\n",
    "        nox += fp['climb'].get(k, 0)\n",
    "    for k in ['NOx_descent_ceil', 'NOx_descent_24_15', 'NOx_descent_15_10', 'NOx_descent_10_5', 'NOx_descent_5_0']:\n",
    "        nox += fp['descent'].get(k, 0)\n",
    "    nox += fp.get('cruise', {}).get('NOx_cruise', 0)\n",
    "    total_nox.append(nox)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(labels, total_nox, color='purple')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Total NOx Emissions (g)')\n",
    "plt.title(f'Total NOx Emissions for Various Altitudes and Distances ({aircraft_typecode})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024efcdd",
   "metadata": {},
   "source": [
    "### Visualize NOx Emissions by Flight Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04149b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize NOx emissions for each climb and descent phase\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "climb_nox_labels = ['Climb 0-5', 'Climb 5-10', 'Climb 10-15', 'Climb 15-24', 'Climb Ceil']\n",
    "climb_nox_keys = ['NOx_climb_0_5', 'NOx_climb_5_10', 'NOx_climb_10_15', 'NOx_climb_15_24', 'NOx_climb_ceil']\n",
    "climb_nox_values = [flightpath['climb'].get(k, 0) for k in climb_nox_keys]\n",
    "\n",
    "descent_nox_labels = ['Descent Ceil', 'Descent 24-15', 'Descent 15-10', 'Descent 10-5', 'Descent 5-0']\n",
    "descent_nox_keys = ['NOx_descent_ceil', 'NOx_descent_24_15', 'NOx_descent_15_10', 'NOx_descent_10_5', 'NOx_descent_5_0']\n",
    "descent_nox_values = [flightpath['descent'].get(k, 0) for k in descent_nox_keys]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "bars_climb = plt.bar(climb_nox_labels, climb_nox_values, color='deepskyblue')\n",
    "plt.xlabel('Climb Segments')\n",
    "plt.ylabel('NOx Emissions (g)')\n",
    "plt.title(f'NOx Emissions During Climb ({aircraft_typecode})')\n",
    "for bar, val in zip(bars_climb, climb_nox_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{val:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "bars_descent = plt.bar(descent_nox_labels, descent_nox_values, color='orange')\n",
    "plt.xlabel('Descent Segments')\n",
    "plt.ylabel('NOx Emissions (g)')\n",
    "plt.title(f'NOx Emissions During Descent ({aircraft_typecode})')\n",
    "for bar, val in zip(bars_descent, descent_nox_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{val:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cumulative NOx Emissions vs. Flight Distance\n",
    "num_flightpaths = 25\n",
    "flight_distances = np.linspace(200, 5000, num_flightpaths)\n",
    "cumulative_nox = []\n",
    "for d_km in flight_distances:\n",
    "    fp = generate_flightpath(aircraft_typecode, d_km, performance_and_emissions_model)\n",
    "    total_nox = 0\n",
    "    for k in ['NOx_climb_0_5', 'NOx_climb_5_10', 'NOx_climb_10_15', 'NOx_climb_15_24', 'NOx_climb_ceil']:\n",
    "        total_nox += fp['climb'].get(k, 0)\n",
    "    for k in ['NOx_descent_ceil', 'NOx_descent_24_15', 'NOx_descent_15_10', 'NOx_descent_10_5', 'NOx_descent_5_0']:\n",
    "        total_nox += fp['descent'].get(k, 0)\n",
    "    total_nox += fp.get('cruise', {}).get('NOx_cruise', 0)\n",
    "    cumulative_nox.append(total_nox)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(flight_distances, cumulative_nox, marker='o', color='purple')\n",
    "plt.xlabel('Flight Distance (km)')\n",
    "plt.ylabel('Cumulative NOx Emissions (g)')\n",
    "plt.title(f'Cumulative NOx Emissions vs. Flight Distance ({aircraft_typecode})')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893eafcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantaneous NOx flux vs. flight distance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Build lists for distance and NOx flux (g/s) at each segment\n",
    "segment_distances = [0]\n",
    "segment_nox_flux = []\n",
    "\n",
    "# Climb segments\n",
    "climb_keys = [\n",
    "    ('s_climb_0_5', 't_climb_0_5', 'NOx_climb_0_5'),\n",
    "    ('s_climb_5_10', 't_climb_5_10', 'NOx_climb_5_10'),\n",
    "    ('s_climb_10_15', 't_climb_10_15', 'NOx_climb_10_15'),\n",
    "    ('s_climb_15_24', 't_climb_15_24', 'NOx_climb_15_24'),\n",
    "    ('s_climb_ceil', 't_climb_ceil', 'NOx_climb_ceil'),\n",
    "]\n",
    "cum_dist = 0\n",
    "for s_key, t_key, nox_key in climb_keys:\n",
    "    s = flightpath['climb'].get(s_key, 0)\n",
    "    t = flightpath['climb'].get(t_key, 0)\n",
    "    nox = flightpath['climb'].get(nox_key, 0)\n",
    "    if t > 0 and s > 0:\n",
    "        cum_dist += s\n",
    "        segment_distances.append(cum_dist)\n",
    "        segment_nox_flux.append(nox / t)\n",
    "\n",
    "# Cruise segment (if available)\n",
    "s_cruise = flightpath['cruise'].get('s_cruise', 0)\n",
    "t_cruise = flightpath['cruise'].get('t_cruise', 0)\n",
    "nox_cruise = flightpath['cruise'].get('NOx_cruise', 0)\n",
    "if t_cruise > 0 and s_cruise > 0:\n",
    "    cum_dist += s_cruise\n",
    "    segment_distances.append(cum_dist)\n",
    "    segment_nox_flux.append(nox_cruise / t_cruise if t_cruise > 0 else 0)\n",
    "\n",
    "# Descent segments\n",
    "descent_keys = [\n",
    "    ('s_descent_ceil', 't_descent_ceil', 'NOx_descent_ceil'),\n",
    "    ('s_descent_24_15', 't_descent_24_15', 'NOx_descent_24_15'),\n",
    "    ('s_descent_15_10', 't_descent_15_10', 'NOx_descent_15_10'),\n",
    "    ('s_descent_10_5', 't_descent_10_5', 'NOx_descent_10_5'),\n",
    "    ('s_descent_5_0', 't_descent_5_0', 'NOx_descent_5_0'),\n",
    "]\n",
    "for s_key, t_key, nox_key in descent_keys:\n",
    "    s = flightpath['descent'].get(s_key, 0)\n",
    "    t = flightpath['descent'].get(t_key, 0)\n",
    "    nox = flightpath['descent'].get(nox_key, 0)\n",
    "    if t > 0 and s > 0:\n",
    "        cum_dist += s\n",
    "        segment_distances.append(cum_dist)\n",
    "        segment_nox_flux.append(nox / t)\n",
    "\n",
    "# Ensure x and y have the same length by removing the initial 0 from segment_distances.\n",
    "segment_distances = segment_distances[1:]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.step(segment_distances, segment_nox_flux, where='post', marker='o')\n",
    "plt.xlabel('Cumulative Flight Distance (meters)')\n",
    "plt.ylabel('Instantaneous NOx Flux (g/s)')\n",
    "plt.title(f'Instantaneous NOx Flux vs. Flight Distance ({aircraft_typecode})')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c044cc",
   "metadata": {},
   "source": [
    "## Model Comparison with 5-Fold Cross-Validation, Hyperparameter Tuning, and Model Saving\n",
    "This section compares XGBoost, polynomial regression, and nonlinear curve fitting for NOx flux prediction, using 5-fold cross-validation and hyperparameter tuning. Only typecodes present in the `performance_and_emissions_model` database are included. Trained models and their best parameters are saved to disk for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433e6ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "'''bnum_altitudes = 6; num_distances = 12\n",
    "typecodes = performance_and_emissions_model['typecode'].unique()\n",
    "model_save_dir = 'saved_models_nox_flux'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "def get_X_y(typecode):\n",
    "    max_dist_row = max_distances[max_distances['typecode'] == typecode]\n",
    "    if max_dist_row.empty: return None, None\n",
    "    max_distance_km = max_dist_row['max_distance_km'].values[0]\n",
    "    max_cruise_altitude_ft = performance_and_emissions_model.loc[performance_and_emissions_model['typecode'] == typecode, 'cruise_Ceiling'].max() * 1e2\n",
    "    if pd.isnull(max_cruise_altitude_ft) or pd.isnull(max_distance_km): return None, None\n",
    "\n",
    "    cruise_altitudes_ft = np.linspace(18000, max_cruise_altitude_ft, num_altitudes)\n",
    "    flight_distances_km = np.logspace(np.log10(200), np.log10(max_distance_km), num_distances)\n",
    "    X, y = [], []\n",
    "    for alt_ft in cruise_altitudes_ft:\n",
    "        for d_km in flight_distances_km:\n",
    "            try:\n",
    "                fp = generate_flightpath(typecode, d_km, performance_and_emissions_model, cruise_altitude_ft=alt_ft)\n",
    "                total_nox = 0; total_time = 0\n",
    "                for nox_key, t_key in zip(['NOx_climb_0_5','NOx_climb_5_10','NOx_climb_10_15','NOx_climb_15_24','NOx_climb_ceil'],['t_climb_0_5','t_climb_5_10','t_climb_10_15','t_climb_15_24','t_climb_ceil']):\n",
    "                    total_nox += fp.get('climb', {}).get(nox_key, 0)\n",
    "                    total_time += fp.get('climb', {}).get(t_key, 0)\n",
    "                for nox_key, t_key in zip(['NOx_descent_ceil','NOx_descent_24_15','NOx_descent_15_10','NOx_descent_10_5','NOx_descent_5_0'],['t_descent_ceil','t_descent_24_15','t_descent_15_10','t_descent_10_5','t_descent_5_0']):\n",
    "                    total_nox += fp.get('descent', {}).get(nox_key, 0)\n",
    "                    total_time += fp.get('descent', {}).get(t_key, 0)\n",
    "                total_nox += fp.get('cruise', {}).get('NOx_cruise', 0)\n",
    "                total_time += fp.get('cruise', {}).get('t_cruise', 0)\n",
    "                if total_time > 0:\n",
    "                    mean_nox_flux = total_nox / total_time\n",
    "                    X.append([d_km, alt_ft])\n",
    "                    y.append(mean_nox_flux)\n",
    "            except Exception: continue\n",
    "    X = np.array(X); y = np.array(y)\n",
    "    if len(y) < 8: return None, None\n",
    "    return X, y\n",
    "\n",
    "# XGBoost with 5-fold CV and Grid Search\n",
    "xgb_results = {}\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.05, 0.1, 0.2]\n",
    "}\n",
    "for typecode in typecodes:\n",
    "    X, y = get_X_y(typecode)\n",
    "    if X is None: continue\n",
    "    model = XGBRegressor(random_state=42)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    grid = GridSearchCV(model, xgb_param_grid, cv=kf, scoring='r2', n_jobs=-1)\n",
    "    grid.fit(X, y)\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = cross_val_predict(best_model, X, y, cv=kf)\n",
    "    r2s = cross_val_score(best_model, X, y, cv=kf, scoring='r2')\n",
    "    rmses = []\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        best_model.fit(X[train_idx], y[train_idx])\n",
    "        y_test_pred = best_model.predict(X[test_idx])\n",
    "        rmses.append(np.sqrt(mean_squared_error(y[test_idx], y_test_pred)))\n",
    "    xgb_results[typecode] = {\n",
    "        'r2_mean': np.mean(r2s), 'r2_std': np.std(r2s),\n",
    "        'rmse_mean': np.mean(rmses), 'rmse_std': np.std(rmses),\n",
    "        'n_samples': len(y), 'best_params': grid.best_params_\n",
    "    }\n",
    "    # Save model\n",
    "    with open(os.path.join(model_save_dir, f'xgb_{typecode}.pkl'), 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    print(f\"[XGBoost CV] {typecode}: R2={np.mean(r2s):.3f}±{np.std(r2s):.3f}, RMSE={np.mean(rmses):.3f}±{np.std(rmses):.3f}, N={len(y)}, Best={grid.best_params_}\")\n",
    "\n",
    "# Polynomial Regression (degree 2) with 5-fold CV and Ridge/Lasso/ElasticNet Grid Search\n",
    "poly_results = {}\n",
    "from sklearn.pipeline import Pipeline\n",
    "poly_param_grid = {\n",
    "    'regressor': [Ridge(), Lasso(), ElasticNet()],\n",
    "    'regressor__alpha': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "for typecode in typecodes:\n",
    "    X, y = get_X_y(typecode)\n",
    "    if X is None: continue\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    X_poly = poly.fit_transform(X_scaled)\n",
    "    pipe = Pipeline([\n",
    "        ('regressor', Ridge())\n",
    "    ])\n",
    "    grid = GridSearchCV(pipe, poly_param_grid, cv=5, scoring='r2', n_jobs=-1, error_score='raise')\n",
    "    grid.fit(X_poly, y)\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = cross_val_predict(best_model, X_poly, y, cv=5)\n",
    "    r2s = cross_val_score(best_model, X_poly, y, cv=5, scoring='r2')\n",
    "    rmses = []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_idx, test_idx in kf.split(X_poly):\n",
    "        best_model.fit(X_poly[train_idx], y[train_idx])\n",
    "        y_test_pred = best_model.predict(X_poly[test_idx])\n",
    "        rmses.append(np.sqrt(mean_squared_error(y[test_idx], y_test_pred)))\n",
    "    poly_results[typecode] = {\n",
    "        'r2_mean': np.mean(r2s), 'r2_std': np.std(r2s),\n",
    "        'rmse_mean': np.mean(rmses), 'rmse_std': np.std(rmses),\n",
    "        'n_samples': len(y), 'best_params': grid.best_params_\n",
    "    }\n",
    "    # Save model and scaler/polynomial\n",
    "    with open(os.path.join(model_save_dir, f'poly2_{typecode}.pkl'), 'wb') as f:\n",
    "        pickle.dump({'model': best_model, 'scaler': scaler, 'poly': poly}, f)\n",
    "    print(f\"[Poly2 CV] {typecode}: R2={np.mean(r2s):.3f}±{np.std(r2s):.3f}, RMSE={np.mean(rmses):.3f}±{np.std(rmses):.3f}, N={len(y)}, Best={grid.best_params_}\")\n",
    "\n",
    "# Nonlinear Curve Fitting (Power Law) with 5-fold CV and Model Saving\n",
    "def powerlaw_func(X, a, b, c):\n",
    "    d, h = X\n",
    "    return a * np.power(d, b) * np.power(h, c)\n",
    "nl_results = {}\n",
    "for typecode in typecodes:\n",
    "    X, y = get_X_y(typecode)\n",
    "    if X is None: continue\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    r2s = []; rmses = []; params_list = []\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        try:\n",
    "            popt, _ = curve_fit(lambda X1, a, b, c: powerlaw_func((X1[:,0], X1[:,1]), a, b, c), X[train_idx], y[train_idx], p0=[1,1,1], maxfev=10000)\n",
    "            y_test_pred = powerlaw_func((X[test_idx,0], X[test_idx,1]), *popt)\n",
    "            r2s.append(r2_score(y[test_idx], y_test_pred))\n",
    "            rmses.append(np.sqrt(mean_squared_error(y[test_idx], y_test_pred)))\n",
    "            params_list.append(popt)\n",
    "        except Exception as e:\n",
    "            r2s.append(np.nan); rmses.append(np.nan); params_list.append([np.nan, np.nan, np.nan])\n",
    "    # Save mean params for this typecode\n",
    "    mean_params = np.nanmean(np.array(params_list), axis=0)\n",
    "    nl_results[typecode] = {'r2_mean': np.nanmean(r2s), 'r2_std': np.nanstd(r2s), 'rmse_mean': np.nanmean(rmses), 'rmse_std': np.nanstd(rmses), 'n_samples': len(y), 'params': mean_params}\n",
    "    with open(os.path.join(model_save_dir, f'nonlinear_{typecode}.pkl'), 'wb') as f:\n",
    "        pickle.dump({'params': mean_params}, f)\n",
    "    print(f\"[Nonlinear CV] {typecode}: R2={np.nanmean(r2s):.3f}±{np.nanstd(r2s):.3f}, RMSE={np.nanmean(rmses):.3f}±{np.nanstd(rmses):.3f}, N={len(y)}, Params={mean_params}\")\n",
    "\n",
    "# Summary Table: Model Comparison for Each Typecode\n",
    "print(\"\\nTypecode | XGBoost R2±std | Poly2 R2±std | Nonlinear R2±std | XGBoost RMSE±std | Poly2 RMSE±std | Nonlinear RMSE±std\")\n",
    "for tc in typecodes:\n",
    "    xg = xgb_results.get(tc, {})\n",
    "    pl = poly_results.get(tc, {})\n",
    "    nl = nl_results.get(tc, {})\n",
    "    print(f\"{tc:8} | {xg.get('r2_mean',float('nan')):.3f}±{xg.get('r2_std',float('nan')):.3f} | {pl.get('r2_mean',float('nan')):.3f}±{pl.get('r2_std',float('nan')):.3f} | {nl.get('r2_mean',float('nan')):.3f}±{nl.get('r2_std',float('nan')):.3f} | {xg.get('rmse_mean',float('nan')):.3f}±{xg.get('rmse_std',float('nan')):.3f} | {pl.get('rmse_mean',float('nan')):.3f}±{pl.get('rmse_std',float('nan')):.3f} | {nl.get('rmse_mean',float('nan')):.3f}±{nl.get('rmse_std',float('nan')):.3f}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0764e2",
   "metadata": {},
   "source": [
    "## Gridded NOx Emissions for January 2024\n",
    "This section generates a 3D grid (0.5° lat × 0.5° lon × 1000 ft altitude bins, up to 16,800 m/55,000 ft) of average NOx flux (kg/month) for all flights in January 2024, using the best (lowest RMSE) model for each aircraft type. Each flight's NOx is distributed along its trajectory, and emissions in each grid cell are averaged over the month, accounting for a 2-day atmospheric removal timescale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21570cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omg28/.conda/envs/traffic/lib/python3.12/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "Flights: 100%|██████████| 1000/1000 [00:12<00:00, 78.20it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch/omg28/Data//emissions/2023-01-01_to_2023-01-31_NOx_nowar.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 96\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lat_idx, lon_idx, alt_idx, value \u001b[38;5;129;01min\u001b[39;00m all_updates:\n\u001b[32m     94\u001b[39m     nox_grid[lat_idx, lon_idx, alt_idx] += value\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/emissions/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstart_time_simple_loop\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_to_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstop_time_simple_loop\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_NOx_nowar.npy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnox_grid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Optionally: Save as NetCDF or CSV for further analysis\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/traffic/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:577\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(file, arr, allow_pickle, fix_imports)\u001b[39m\n\u001b[32m    575\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file.endswith(\u001b[33m'\u001b[39m\u001b[33m.npy\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    576\u001b[39m         file = file + \u001b[33m'\u001b[39m\u001b[33m.npy\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m     file_ctx = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[32m    580\u001b[39m     arr = np.asanyarray(arr)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/scratch/omg28/Data//emissions/2023-01-01_to_2023-01-31_NOx_nowar.npy'"
     ]
    }
   ],
   "source": [
    "performance_and_emissions_model = pd.read_pickle('performance_and_emissions_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779cba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the NOx emissions grid\n",
    "nox_grid = np.load('nox_emissions_grid_jan2024.npy')\n",
    "\n",
    "# Define the grid bins (must match those used in the emissions calculation)\n",
    "lat_bins = np.arange(-90, 90.1, 0.5)\n",
    "alt_bins_ft = np.arange(0, 55001, 1000)\n",
    "alt_bins_m = alt_bins_ft * 0.3048\n",
    "\n",
    "# Sum over longitude to get a 2D array: (latitude, altitude)\n",
    "nox_lat_alt = np.sum(nox_grid, axis=1)  # shape: (nlat, nalt)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(\n",
    "    nox_lat_alt.T, \n",
    "    aspect='auto', \n",
    "    origin='lower',\n",
    "    extent=[lat_bins[0], lat_bins[-2], alt_bins_m[0], alt_bins_m[-2]],\n",
    "    cmap='inferno'\n",
    ")\n",
    "plt.colorbar(label='NOx Emissions (kg/month)')\n",
    "plt.xlabel('Latitude (degrees)')\n",
    "plt.ylabel('Altitude (meters)')\n",
    "plt.title('NOx Emissions Heatmap (Jan 2024): Altitude vs Latitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a4e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the NOx emissions grid (already loaded as nox_grid in cell 43)\n",
    "# nox_grid shape: (nlat, nlon, nalt)\n",
    "\n",
    "# Sum over altitude to get total NOx flux per (lat, lon) cell\n",
    "nox_lat_lon = np.sum(nox_grid, axis=2)  # shape: (nlat, nlon)\n",
    "\n",
    "# Define the grid bins (must match those used in the emissions calculation)\n",
    "lat_bins = np.arange(-90, 90.1, 0.5)\n",
    "lon_bins = np.arange(-180, 180.1, 0.5)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(\n",
    "    nox_lat_lon,\n",
    "    extent=[lon_bins[0], lon_bins[-2], lat_bins[0], lat_bins[-2]],\n",
    "    origin='lower',\n",
    "    aspect='auto',\n",
    "    cmap='inferno'\n",
    ")\n",
    "plt.colorbar(label='NOx Emissions (kg/month)')\n",
    "plt.xlabel('Longitude (degrees)')\n",
    "plt.ylabel('Latitude (degrees)')\n",
    "plt.title('Average Monthly NOx Flux (kg/month) vs Latitude and Longitude (Jan 2024)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a1d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Helper to extract R^2 values from results dict\n",
    "def extract_r2(results):\n",
    "    return np.array([v['r2_mean'] for v in results.values() if 'r2_mean' in v and not np.isnan(v['r2_mean'])])\n",
    "\n",
    "# Load the results dictionaries if not already in memory\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "model_dir = 'saved_models_nox_flux'\n",
    "\n",
    "# Load the results dictionaries from pickle files\n",
    "with open(os.path.join(model_dir, 'xgb_results.pkl'), 'rb') as f:\n",
    "    xgb_results = pickle.load(f)\n",
    "with open(os.path.join(model_dir, 'poly_results.pkl'), 'rb') as f:\n",
    "    poly_results = pickle.load(f)\n",
    "with open(os.path.join(model_dir, 'nl_results.pkl'), 'rb') as f:\n",
    "    nl_results = pickle.load(f)\n",
    "\n",
    "xgb_r2 = extract_r2(xgb_results)\n",
    "ridge_r2 = extract_r2(poly_results)\n",
    "nonlinear_r2 = extract_r2(nl_results)\n",
    "\n",
    "print(\"XGBoost:   avg R^2 = {:.3f}, min R^2 = {:.3f}\".format(np.mean(xgb_r2), np.min(xgb_r2)))\n",
    "print(\"Ridge:     avg R^2 = {:.3f}, min R^2 = {:.3f}\".format(np.mean(ridge_r2), np.min(ridge_r2)))\n",
    "print(\"Nonlinear: avg R^2 = {:.3f}, min R^2 = {:.3f}\".format(np.mean(nonlinear_r2), np.min(nonlinear_r2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b67bd8",
   "metadata": {},
   "source": [
    "### Output\n",
    "- The file `nox_emissions_grid_jan2024.npy` contains a 3D numpy array of average NOx flux (kg/month) for each grid cell (lat, lon, alt) for January 2024.\n",
    "- Each cell represents the average NOx present, accounting for a 2-day removal timescale.\n",
    "- You can convert this to other formats (e.g., NetCDF) for visualization or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17005fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "model = pd.read_pickle('saved_models_nox_flux/xgb_A320.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
